<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lesson 9 Regularized Regression | 581092 Data Science</title>
  <meta name="description" content="Welcome to most important course you‚Äôll ever take: Data Science üôÑ" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Lesson 9 Regularized Regression | 581092 Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Welcome to most important course you‚Äôll ever take: Data Science üôÑ" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lesson 9 Regularized Regression | 581092 Data Science" />
  
  <meta name="twitter:description" content="Welcome to most important course you‚Äôll ever take: Data Science üôÑ" />
  

<meta name="author" content="M Loecher" />


<meta name="date" content="2021-09-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification.html"/>
<link rel="next" href="trees.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">BIPM Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-components"><i class="fa fa-check"></i>Course components</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#grading"><i class="fa fa-check"></i>Grading</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-and-environments"><i class="fa fa-check"></i>Software and Environments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="statistical-thinking-i.html"><a href="statistical-thinking-i.html"><i class="fa fa-check"></i><b>1</b> Statistical Thinking I</a>
<ul>
<li class="chapter" data-level="1.1" data-path="statistical-thinking-i.html"><a href="statistical-thinking-i.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="statistical-thinking-i.html"><a href="statistical-thinking-i.html#contingency-tables-as-simple-models"><i class="fa fa-check"></i><b>1.2</b> Contingency Tables as simple models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sampling-uncertainty.html"><a href="sampling-uncertainty.html"><i class="fa fa-check"></i><b>2</b> Sampling Uncertainty</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sampling-uncertainty.html"><a href="sampling-uncertainty.html#ab-testing"><i class="fa fa-check"></i><b>2.1</b> A/B Testing</a></li>
<li class="chapter" data-level="2.2" data-path="sampling-uncertainty.html"><a href="sampling-uncertainty.html#distributions"><i class="fa fa-check"></i><b>2.2</b> Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="sampling-uncertainty.html"><a href="sampling-uncertainty.html#hacker-statistic"><i class="fa fa-check"></i><b>2.3</b> Hacker Statistic</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>3</b> Testing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="testing.html"><a href="testing.html#hypothesis-tests"><i class="fa fa-check"></i><b>3.1</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="3.2" data-path="testing.html"><a href="testing.html#parametric-tests"><i class="fa fa-check"></i><b>3.2</b> Parametric Tests</a></li>
<li class="chapter" data-level="3.3" data-path="testing.html"><a href="testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>3.3</b> Non parametric Tests</a></li>
<li class="chapter" data-level="3.4" data-path="testing.html"><a href="testing.html#bootstrap-hypothesis-tests"><i class="fa fa-check"></i><b>3.4</b> Bootstrap Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="testing.html"><a href="testing.html#a-two-sample-bootstrap-hypothesis-test-for-difference-of-means"><i class="fa fa-check"></i><b>3.4.1</b> A two-sample bootstrap hypothesis test for difference of means</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="advanced-tests.html"><a href="advanced-tests.html"><i class="fa fa-check"></i><b>4</b> Advanced Tests</a>
<ul>
<li class="chapter" data-level="4.1" data-path="advanced-tests.html"><a href="advanced-tests.html#permutation-2-sample-test"><i class="fa fa-check"></i><b>4.1</b> Permutation 2-sample test</a></li>
<li class="chapter" data-level="4.2" data-path="advanced-tests.html"><a href="advanced-tests.html#sample-t-test"><i class="fa fa-check"></i><b>4.2</b> 2-sample t test</a></li>
<li class="chapter" data-level="4.3" data-path="advanced-tests.html"><a href="advanced-tests.html#random-walks"><i class="fa fa-check"></i><b>4.3</b> Random Walks</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="advanced-tests.html"><a href="advanced-tests.html#the-sqrtn-law-again"><i class="fa fa-check"></i><b>4.3.1</b> The <span class="math inline">\(\sqrt{n}\)</span> law again !</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-linear-regressio.html"><a href="simple-linear-regressio.html"><i class="fa fa-check"></i><b>5</b> Simple Linear Regressio</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-linear-regressio.html"><a href="simple-linear-regressio.html#loss-functions"><i class="fa fa-check"></i><b>5.1</b> Loss Functions</a></li>
<li class="chapter" data-level="5.2" data-path="simple-linear-regressio.html"><a href="simple-linear-regressio.html#least-squares"><i class="fa fa-check"></i><b>5.2</b> Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#statsmodels"><i class="fa fa-check"></i><b>6.1</b> Statsmodels</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>6.2</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#categorical-variables"><i class="fa fa-check"></i><b>6.3</b> Categorical Variables</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interactions"><i class="fa fa-check"></i><b>6.4</b> Interactions</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#non-linear-relationships"><i class="fa fa-check"></i><b>6.5</b> Non-linear relationships</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html"><i class="fa fa-check"></i><b>7</b> Advanced Linear Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#dummy-coding"><i class="fa fa-check"></i><b>7.1</b> Dummy Coding</a></li>
<li class="chapter" data-level="7.2" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#overfitting"><i class="fa fa-check"></i><b>7.2</b> Overfitting</a></li>
<li class="chapter" data-level="7.3" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.3</b> Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>8</b> Classification</a>
<ul>
<li class="chapter" data-level="8.1" data-path="classification.html"><a href="classification.html#datasets"><i class="fa fa-check"></i><b>8.1</b> Datasets</a></li>
<li class="chapter" data-level="8.2" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>8.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="8.3" data-path="classification.html"><a href="classification.html#other-classifiers"><i class="fa fa-check"></i><b>8.3</b> Other Classifiers</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="classification.html"><a href="classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3.1</b> K Nearest Neighbors</a></li>
<li class="chapter" data-level="8.3.2" data-path="classification.html"><a href="classification.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>8.3.2</b> Multinomial Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regularized-regression.html"><a href="regularized-regression.html"><i class="fa fa-check"></i><b>9</b> Regularized Regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="regularized-regression.html"><a href="regularized-regression.html#other-classifiers-1"><i class="fa fa-check"></i><b>9.1</b> Other Classifiers</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="regularized-regression.html"><a href="regularized-regression.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>9.1.1</b> K-Nearest Neighbors (KNN)</a></li>
<li class="chapter" data-level="9.1.2" data-path="regularized-regression.html"><a href="regularized-regression.html#multinomial-logistic-regression-1"><i class="fa fa-check"></i><b>9.1.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="9.1.3" data-path="regularized-regression.html"><a href="regularized-regression.html#roc-curves"><i class="fa fa-check"></i><b>9.1.3</b> ROC Curves</a></li>
<li class="chapter" data-level="9.1.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regularized-regression-1"><i class="fa fa-check"></i><b>9.1.4</b> Regularized Regression</a></li>
<li class="chapter" data-level="9.1.5" data-path="regularized-regression.html"><a href="regularized-regression.html#kaggle"><i class="fa fa-check"></i><b>9.1.5</b> Kaggle</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>10</b> Trees</a>
<ul>
<li class="chapter" data-level="10.1" data-path="trees.html"><a href="trees.html#node-impurity"><i class="fa fa-check"></i><b>10.1</b> Node Impurity</a></li>
<li class="chapter" data-level="10.2" data-path="trees.html"><a href="trees.html#regression-trees"><i class="fa fa-check"></i><b>10.2</b> Regression Trees</a></li>
<li class="chapter" data-level="10.3" data-path="trees.html"><a href="trees.html#classification-trees"><i class="fa fa-check"></i><b>10.3</b> Classification Trees</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="from-trees-to-forests.html"><a href="from-trees-to-forests.html"><i class="fa fa-check"></i><b>11</b> From Trees to Forests</a>
<ul>
<li class="chapter" data-level="11.1" data-path="from-trees-to-forests.html"><a href="from-trees-to-forests.html#classification-tree"><i class="fa fa-check"></i><b>11.1</b> Classification Tree</a></li>
<li class="chapter" data-level="11.2" data-path="from-trees-to-forests.html"><a href="from-trees-to-forests.html#ensembles-of-estimators"><i class="fa fa-check"></i><b>11.2</b> Ensembles of Estimators</a></li>
<li class="chapter" data-level="11.3" data-path="from-trees-to-forests.html"><a href="from-trees-to-forests.html#bagging"><i class="fa fa-check"></i><b>11.3</b> Bagging</a></li>
<li class="chapter" data-level="11.4" data-path="from-trees-to-forests.html"><a href="from-trees-to-forests.html#random-forests"><i class="fa fa-check"></i><b>11.4</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="explainable-ml.html"><a href="explainable-ml.html"><i class="fa fa-check"></i><b>12</b> Explainable ML</a>
<ul>
<li class="chapter" data-level="12.1" data-path="explainable-ml.html"><a href="explainable-ml.html#partial-dependence-plots"><i class="fa fa-check"></i><b>12.1</b> Partial dependence plots</a></li>
<li class="chapter" data-level="12.2" data-path="explainable-ml.html"><a href="explainable-ml.html#shap-values"><i class="fa fa-check"></i><b>12.2</b> SHAP values</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="explainable-ml.html"><a href="explainable-ml.html#titanic"><i class="fa fa-check"></i><b>12.2.1</b> Titanic</a></li>
<li class="chapter" data-level="12.2.2" data-path="explainable-ml.html"><a href="explainable-ml.html#force-plots"><i class="fa fa-check"></i><b>12.2.2</b> Force plots</a></li>
<li class="chapter" data-level="12.2.3" data-path="explainable-ml.html"><a href="explainable-ml.html#tasks-3"><i class="fa fa-check"></i><b>12.2.3</b> Tasks</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">581092 Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularized-regression" class="section level1" number="9">
<h1><span class="header-section-number">Lesson 9</span> Regularized Regression</h1>
<div class="sourceCode" id="cb593"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb593-1"><a href="regularized-regression.html#cb593-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb593-2"><a href="regularized-regression.html#cb593-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb593-3"><a href="regularized-regression.html#cb593-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb593-4"><a href="regularized-regression.html#cb593-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb593-5"><a href="regularized-regression.html#cb593-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb593-6"><a href="regularized-regression.html#cb593-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb593-7"><a href="regularized-regression.html#cb593-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb593-8"><a href="regularized-regression.html#cb593-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb593-9"><a href="regularized-regression.html#cb593-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb593-10"><a href="regularized-regression.html#cb593-10" aria-hidden="true" tabindex="-1"></a><span class="co"># %precision 3</span></span></code></pre></div>
<div id="other-classifiers-1" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Other Classifiers</h2>
<div id="k-nearest-neighbors-knn" class="section level3" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> K-Nearest Neighbors (KNN)</h3>
<p><img src="../figures/irisDataCampKNN.png" width=600></p>
<div class="sourceCode" id="cb594"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb594-1"><a href="regularized-regression.html#cb594-1" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb594-2"><a href="regularized-regression.html#cb594-2" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb594-3"><a href="regularized-regression.html#cb594-3" aria-hidden="true" tabindex="-1"></a>knn.fit(iris[<span class="st">&#39;data&#39;</span>], iris[<span class="st">&#39;target&#39;</span>])</span></code></pre></div>
<pre><code>## KNeighborsClassifier(n_neighbors=6)</code></pre>
<p>Task: Split the iris data into training and test. Predict on test</p>
<div class="sourceCode" id="cb596"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb596-1"><a href="regularized-regression.html#cb596-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test set</span></span>
<span id="cb596-2"><a href="regularized-regression.html#cb596-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris[<span class="st">&#39;data&#39;</span>]</span>
<span id="cb596-3"><a href="regularized-regression.html#cb596-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> iris[<span class="st">&#39;target&#39;</span>]</span>
<span id="cb596-4"><a href="regularized-regression.html#cb596-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb596-5"><a href="regularized-regression.html#cb596-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb596-6"><a href="regularized-regression.html#cb596-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a k-NN classifier with 7 neighbors: knn</span></span>
<span id="cb596-7"><a href="regularized-regression.html#cb596-7" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb596-8"><a href="regularized-regression.html#cb596-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb596-9"><a href="regularized-regression.html#cb596-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the classifier to the training data</span></span>
<span id="cb596-10"><a href="regularized-regression.html#cb596-10" aria-hidden="true" tabindex="-1"></a>knn.fit(X_train, y_train)</span>
<span id="cb596-11"><a href="regularized-regression.html#cb596-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb596-12"><a href="regularized-regression.html#cb596-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the accuracy</span></span></code></pre></div>
<pre><code>## KNeighborsClassifier(n_neighbors=6)</code></pre>
<div class="sourceCode" id="cb598"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb598-1"><a href="regularized-regression.html#cb598-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(knn.score(X_test, y_test))</span></code></pre></div>
<pre><code>## 1.0</code></pre>
<!-- #region -->
<p>Simple Idea, no modeling assumptions at all !!
Think about the following:</p>
<ul>
<li>What is ‚Äúthe model,‚Äù i.e.¬†what needs to be stored ? (coefficients, functions, ‚Ä¶)</li>
<li>What is the model complexity ?</li>
<li>Does this only work for classification ? What would be the regression analogy ?</li>
<li>What improvements could we make to the simple idea ?</li>
<li>In the modeling world:
<ul>
<li>linear ?</li>
<li>local vs.¬†global</li>
<li>memory/CPU requirements</li>
<li>wide versus tall data ?</li>
</ul></li>
</ul>
<div id="handwritten-digits" class="section level4" number="9.1.1.1">
<h4><span class="header-section-number">9.1.1.1</span> Handwritten Digits</h4>
<!-- #endregion -->
<div class="sourceCode" id="cb600"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb600-1"><a href="regularized-regression.html#cb600-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits()</span>
<span id="cb600-2"><a href="regularized-regression.html#cb600-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb600-3"><a href="regularized-regression.html#cb600-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb600-4"><a href="regularized-regression.html#cb600-4" aria-hidden="true" tabindex="-1"></a><span class="co">#how can I improve the plots ? (i..e no margins, box around plot...)</span></span>
<span id="cb600-5"><a href="regularized-regression.html#cb600-5" aria-hidden="true" tabindex="-1"></a>plt.figure(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## &lt;Figure size 700x500 with 0 Axes&gt;</code></pre>
<div class="sourceCode" id="cb602"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb602-1"><a href="regularized-regression.html#cb602-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="dv">10</span>)<span class="op">+</span><span class="dv">1</span>:</span>
<span id="cb602-2"><a href="regularized-regression.html#cb602-2" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">5</span>, i)</span>
<span id="cb602-3"><a href="regularized-regression.html#cb602-3" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb602-4"><a href="regularized-regression.html#cb602-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#plt.gray()</span></span>
<span id="cb602-5"><a href="regularized-regression.html#cb602-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#plt.matshow(digits.images[i-1])</span></span>
<span id="cb602-6"><a href="regularized-regression.html#cb602-6" aria-hidden="true" tabindex="-1"></a>    plt.imshow(digits.images[i<span class="op">-</span><span class="dv">1</span>], cmap<span class="op">=</span>plt.cm.gray_r, interpolation<span class="op">=</span><span class="st">&#39;nearest&#39;</span>)</span></code></pre></div>
<pre><code>## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10b43b9e8&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10b45fc88&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10b950e80&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10b8fecf8&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10b92f860&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10b487d30&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10be52b70&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10be52be0&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb10b6babe0&gt;
## &lt;AxesSubplot:&gt;
## (0.0, 1.0, 0.0, 1.0)
## &lt;matplotlib.image.AxesImage object at 0x7fb109dbda58&gt;</code></pre>
<div class="sourceCode" id="cb604"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb604-1"><a href="regularized-regression.html#cb604-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="BIPM_DS_files/figure-html/unnamed-chunk-224-1.png" width="672" /></p>
<div class="sourceCode" id="cb605"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb605-1"><a href="regularized-regression.html#cb605-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create feature and target arrays</span></span>
<span id="cb605-2"><a href="regularized-regression.html#cb605-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.data</span>
<span id="cb605-3"><a href="regularized-regression.html#cb605-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb605-4"><a href="regularized-regression.html#cb605-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-5"><a href="regularized-regression.html#cb605-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test set</span></span>
<span id="cb605-6"><a href="regularized-regression.html#cb605-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y)</span>
<span id="cb605-7"><a href="regularized-regression.html#cb605-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-8"><a href="regularized-regression.html#cb605-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a k-NN classifier with 7 neighbors: knn</span></span>
<span id="cb605-9"><a href="regularized-regression.html#cb605-9" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb605-10"><a href="regularized-regression.html#cb605-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-11"><a href="regularized-regression.html#cb605-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the classifier to the training data</span></span>
<span id="cb605-12"><a href="regularized-regression.html#cb605-12" aria-hidden="true" tabindex="-1"></a>knn.fit(X_train, y_train)</span>
<span id="cb605-13"><a href="regularized-regression.html#cb605-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-14"><a href="regularized-regression.html#cb605-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the accuracy</span></span></code></pre></div>
<pre><code>## KNeighborsClassifier(n_neighbors=7)</code></pre>
<div class="sourceCode" id="cb607"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb607-1"><a href="regularized-regression.html#cb607-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(knn.score(X_test, y_test))</span></code></pre></div>
<pre><code>## 0.9833333333333333</code></pre>
<div class="sourceCode" id="cb609"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb609-1"><a href="regularized-regression.html#cb609-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Big Confusion Matrix</span></span>
<span id="cb609-2"><a href="regularized-regression.html#cb609-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> knn.predict(X_test)</span>
<span id="cb609-3"><a href="regularized-regression.html#cb609-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb609-4"><a href="regularized-regression.html#cb609-4" aria-hidden="true" tabindex="-1"></a>pd.crosstab(preds,y_test)</span></code></pre></div>
<pre><code>## col_0   0   1   2   3   4   5   6   7   8   9
## row_0                                        
## 0      36   0   0   0   0   0   0   0   0   0
## 1       0  36   0   0   0   0   0   0   2   0
## 2       0   0  35   0   0   0   0   0   0   0
## 3       0   0   0  37   0   0   0   0   0   0
## 4       0   0   0   0  36   0   0   0   0   1
## 5       0   0   0   0   0  37   0   0   0   0
## 6       0   0   0   0   0   0  35   0   0   0
## 7       0   0   0   0   0   0   0  36   1   0
## 8       0   0   0   0   0   0   1   0  32   1
## 9       0   0   0   0   0   0   0   0   0  34</code></pre>
<p><strong>Task</strong></p>
<p>Construct a model complexity curve for the digits dataset! In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting.</p>
<div class="sourceCode" id="cb611"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb611-1"><a href="regularized-regression.html#cb611-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Setup arrays to store train and test accuracies</span></span>
<span id="cb611-2"><a href="regularized-regression.html#cb611-2" aria-hidden="true" tabindex="-1"></a><span class="co"># neighbors = np.arange(1, 20)</span></span>
<span id="cb611-3"><a href="regularized-regression.html#cb611-3" aria-hidden="true" tabindex="-1"></a><span class="co"># train_accuracy = np.empty(len(neighbors))</span></span>
<span id="cb611-4"><a href="regularized-regression.html#cb611-4" aria-hidden="true" tabindex="-1"></a><span class="co"># test_accuracy = np.empty(len(neighbors))</span></span>
<span id="cb611-5"><a href="regularized-regression.html#cb611-5" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb611-6"><a href="regularized-regression.html#cb611-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # Loop over different values of k</span></span>
<span id="cb611-7"><a href="regularized-regression.html#cb611-7" aria-hidden="true" tabindex="-1"></a><span class="co"># for i, k in enumerate(neighbors):</span></span>
<span id="cb611-8"><a href="regularized-regression.html#cb611-8" aria-hidden="true" tabindex="-1"></a><span class="co">#     # Setup a k-NN Classifier with k neighbors: knn</span></span>
<span id="cb611-9"><a href="regularized-regression.html#cb611-9" aria-hidden="true" tabindex="-1"></a><span class="co">#     knn = KNeighborsClassifier(___)</span></span>
<span id="cb611-10"><a href="regularized-regression.html#cb611-10" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb611-11"><a href="regularized-regression.html#cb611-11" aria-hidden="true" tabindex="-1"></a><span class="co">#     # Fit the classifier to the training data</span></span>
<span id="cb611-12"><a href="regularized-regression.html#cb611-12" aria-hidden="true" tabindex="-1"></a><span class="co">#     ___</span></span>
<span id="cb611-13"><a href="regularized-regression.html#cb611-13" aria-hidden="true" tabindex="-1"></a><span class="co">#     </span></span>
<span id="cb611-14"><a href="regularized-regression.html#cb611-14" aria-hidden="true" tabindex="-1"></a><span class="co">#     #Compute accuracy on the training set</span></span>
<span id="cb611-15"><a href="regularized-regression.html#cb611-15" aria-hidden="true" tabindex="-1"></a><span class="co">#     train_accuracy[i] = ___</span></span>
<span id="cb611-16"><a href="regularized-regression.html#cb611-16" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb611-17"><a href="regularized-regression.html#cb611-17" aria-hidden="true" tabindex="-1"></a><span class="co">#     #Compute accuracy on the testing set</span></span>
<span id="cb611-18"><a href="regularized-regression.html#cb611-18" aria-hidden="true" tabindex="-1"></a><span class="co">#     test_accuracy[i] = ___</span></span>
<span id="cb611-19"><a href="regularized-regression.html#cb611-19" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb611-20"><a href="regularized-regression.html#cb611-20" aria-hidden="true" tabindex="-1"></a><span class="co"># # Generate plot</span></span>
<span id="cb611-21"><a href="regularized-regression.html#cb611-21" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.title(&#39;k-NN: Varying Number of Neighbors&#39;)</span></span>
<span id="cb611-22"><a href="regularized-regression.html#cb611-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(___, label = &#39;Testing Accuracy&#39;)</span></span>
<span id="cb611-23"><a href="regularized-regression.html#cb611-23" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(___, label = &#39;Training Accuracy&#39;)</span></span>
<span id="cb611-24"><a href="regularized-regression.html#cb611-24" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.legend()</span></span>
<span id="cb611-25"><a href="regularized-regression.html#cb611-25" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.xlabel(&#39;Number of Neighbors&#39;)</span></span>
<span id="cb611-26"><a href="regularized-regression.html#cb611-26" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.ylabel(&#39;Accuracy&#39;)</span></span>
<span id="cb611-27"><a href="regularized-regression.html#cb611-27" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code></pre></div>
</div>
</div>
<div id="multinomial-logistic-regression-1" class="section level3" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Multinomial Logistic Regression</h3>
<div class="sourceCode" id="cb612"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb612-1"><a href="regularized-regression.html#cb612-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb612-2"><a href="regularized-regression.html#cb612-2" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb612-3"><a href="regularized-regression.html#cb612-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb612-4"><a href="regularized-regression.html#cb612-4" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">&#39;multinomial&#39;</span>,solver<span class="op">=</span><span class="st">&#39;sag&#39;</span>, max_iter<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb612-5"><a href="regularized-regression.html#cb612-5" aria-hidden="true" tabindex="-1"></a>log_reg.fit(iris[<span class="st">&quot;data&quot;</span>][:,<span class="dv">3</span>:],iris[<span class="st">&quot;target&quot;</span>])</span></code></pre></div>
<pre><code>## LogisticRegression(multi_class=&#39;multinomial&#39;, random_state=42, solver=&#39;sag&#39;)</code></pre>
<div class="sourceCode" id="cb614"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb614-1"><a href="regularized-regression.html#cb614-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> log_reg.predict_proba(iris[<span class="st">&quot;data&quot;</span>][:,<span class="dv">3</span>:])</span>
<span id="cb614-2"><a href="regularized-regression.html#cb614-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb614-3"><a href="regularized-regression.html#cb614-3" aria-hidden="true" tabindex="-1"></a>preds[<span class="dv">1</span>:<span class="dv">5</span>,:]</span></code></pre></div>
<pre><code>## array([[9.259e-01, 7.396e-02, 1.788e-04],
##        [9.259e-01, 7.396e-02, 1.788e-04],
##        [9.259e-01, 7.396e-02, 1.788e-04],
##        [9.259e-01, 7.396e-02, 1.788e-04]])</code></pre>
<p><strong>Task</strong>: Compute a confusion matrix</p>
<p><strong>Further Reading and Explorations:</strong>
* Read about ‚Äúone versus all‚Äù pitted against multinomial. Check out this <a href="plot_logistic_multinomial.ipynb">notebook</a>.
* Read about <a href="http://data.princeton.edu/wws509/stata/mlogit.html">marginal probabilities</a>.</p>
</div>
<div id="roc-curves" class="section level3" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> ROC Curves</h3>
<p>Simplest to go back to 2 labels from lesson 7:</p>
<div class="sourceCode" id="cb616"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb616-1"><a href="regularized-regression.html#cb616-1" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb616-2"><a href="regularized-regression.html#cb616-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris[<span class="st">&quot;data&quot;</span>][:,<span class="dv">3</span>:]</span>
<span id="cb616-3"><a href="regularized-regression.html#cb616-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (iris[<span class="st">&quot;target&quot;</span>]<span class="op">==</span><span class="dv">2</span>).astype(<span class="bu">int</span>)</span>
<span id="cb616-4"><a href="regularized-regression.html#cb616-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test set</span></span>
<span id="cb616-5"><a href="regularized-regression.html#cb616-5" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb616-6"><a href="regularized-regression.html#cb616-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb616-7"><a href="regularized-regression.html#cb616-7" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression()</span>
<span id="cb616-8"><a href="regularized-regression.html#cb616-8" aria-hidden="true" tabindex="-1"></a>log_reg.fit(X_train,y_train)</span></code></pre></div>
<pre><code>## LogisticRegression()</code></pre>
<div class="sourceCode" id="cb618"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb618-1"><a href="regularized-regression.html#cb618-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> log_reg.predict_proba(X_test)</span>
<span id="cb618-2"><a href="regularized-regression.html#cb618-2" aria-hidden="true" tabindex="-1"></a>y_pred_prob <span class="op">=</span> y_pred[:,<span class="dv">1</span>]</span>
<span id="cb618-3"><a href="regularized-regression.html#cb618-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb618-4"><a href="regularized-regression.html#cb618-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb618-5"><a href="regularized-regression.html#cb618-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb618-6"><a href="regularized-regression.html#cb618-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb618-7"><a href="regularized-regression.html#cb618-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred_prob <span class="op">&gt;</span> <span class="fl">0.25</span> ))</span></code></pre></div>
<pre><code>## [[24  8]
##  [ 0 13]]</code></pre>
<div class="sourceCode" id="cb620"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb620-1"><a href="regularized-regression.html#cb620-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred_prob <span class="op">&gt;</span> <span class="fl">0.5</span> ))</span></code></pre></div>
<pre><code>## [[32  0]
##  [ 0 13]]</code></pre>
<div class="sourceCode" id="cb622"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb622-1"><a href="regularized-regression.html#cb622-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred_prob <span class="op">&gt;</span> <span class="fl">0.75</span> ))</span></code></pre></div>
<pre><code>## [[32  0]
##  [ 4  9]]</code></pre>
<!-- #region -->
<p><strong>The need for more sophisticated metrics than accuracy and single thresholding</strong></p>
<p>This is particularly relevant for imbalanced classes, example: Emails</p>
<ul>
<li>Spam classification
<ul>
<li>99% of emails are real; 1% of emails are spam</li>
</ul></li>
<li>Could build a classifier that predicts ALL emails as real
<ul>
<li>99% accurate!</li>
<li>But horrible at actually classifying spam</li>
<li>Fails at its original purpose
<table>
<tr>
<td>
<img src="../figures/DataCampConfusionMatrix.png" width=600>
</td>
<td>
<img src="../figures/ISLR-Table4.7.png" width=600>
</td>
</tr>
</table></li>
</ul></li>
</ul>
<p><strong>Metrics from CM</strong></p>
<ul>
<li>Precision: <span class="math display">\[\frac{TP}{TP+FP}\]</span></li>
<li>Recall: <span class="math display">\[\frac{TP}{TP+FN}\]</span></li>
<li>F1 score:
<span class="math display">\[2 \cdot \frac{precision \cdot recall}{precision + recall}\]</span>
The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. ()</li>
<li>High precision: Not many real emails predicted as spam</li>
<li>High recall: Predicted most spam emails correctly</li>
</ul>
<!-- #endregion -->
<div class="sourceCode" id="cb624"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb624-1"><a href="regularized-regression.html#cb624-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Example:</span></span></code></pre></div>
<div class="sourceCode" id="cb625"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb625-1"><a href="regularized-regression.html#cb625-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_prob <span class="op">&gt;</span> <span class="fl">0.5</span> ))</span></code></pre></div>
<pre><code>##               precision    recall  f1-score   support
## 
##            0       1.00      1.00      1.00        32
##            1       1.00      1.00      1.00        13
## 
##     accuracy                           1.00        45
##    macro avg       1.00      1.00      1.00        45
## weighted avg       1.00      1.00      1.00        45</code></pre>
<p>But we still need to fix a threshold for any of the metrics above to work !</p>
<p>Wouldn‚Äôt it be best to communicate the prediction quality over a wide range (all!) of thresholds and enable the user to choose the most suitable one for his/her application !
That is the idea of the <strong>Receiver Operating Characteristic (ROC)</strong> curve!</p>
<p><img src="../figures/DataCampROCexample.png" width=600></p>
<div class="sourceCode" id="cb627"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb627-1"><a href="regularized-regression.html#cb627-1" aria-hidden="true" tabindex="-1"></a><span class="co">#The pima-indians- diabetes data:</span></span>
<span id="cb627-2"><a href="regularized-regression.html#cb627-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  https://www.kaggle.com/uciml/pima-indians-diabetes-database#diabetes.csv</span></span>
<span id="cb627-3"><a href="regularized-regression.html#cb627-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-4"><a href="regularized-regression.html#cb627-4" aria-hidden="true" tabindex="-1"></a>col_names <span class="op">=</span> [<span class="st">&#39;pregnant&#39;</span>, <span class="st">&#39;glucose&#39;</span>, <span class="st">&#39;bp&#39;</span>, <span class="st">&#39;skin&#39;</span>, <span class="st">&#39;insulin&#39;</span>, <span class="st">&#39;bmi&#39;</span>, <span class="st">&#39;pedigree&#39;</span>, <span class="st">&#39;age&#39;</span>, <span class="st">&#39;label&#39;</span>]</span>
<span id="cb627-5"><a href="regularized-regression.html#cb627-5" aria-hidden="true" tabindex="-1"></a><span class="co"># load dataset</span></span>
<span id="cb627-6"><a href="regularized-regression.html#cb627-6" aria-hidden="true" tabindex="-1"></a>pima <span class="op">=</span> pd.read_csv(<span class="st">&quot;../data/diabetes.csv&quot;</span>, header<span class="op">=</span><span class="va">None</span>, names<span class="op">=</span>col_names,skiprows<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb627-7"><a href="regularized-regression.html#cb627-7" aria-hidden="true" tabindex="-1"></a>pima.head()</span></code></pre></div>
<pre><code>##    pregnant  glucose  bp  skin  insulin   bmi  pedigree  age  label
## 0         6      148  72    35        0  33.6     0.627   50      1
## 1         1       85  66    29        0  26.6     0.351   31      0
## 2         8      183  64     0        0  23.3     0.672   32      1
## 3         1       89  66    23       94  28.1     0.167   21      0
## 4         0      137  40    35      168  43.1     2.288   33      1</code></pre>
<div class="sourceCode" id="cb629"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb629-1"><a href="regularized-regression.html#cb629-1" aria-hidden="true" tabindex="-1"></a>pima.info()</span></code></pre></div>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
## RangeIndex: 768 entries, 0 to 767
## Data columns (total 9 columns):
##  #   Column    Non-Null Count  Dtype  
## ---  ------    --------------  -----  
##  0   pregnant  768 non-null    int64  
##  1   glucose   768 non-null    int64  
##  2   bp        768 non-null    int64  
##  3   skin      768 non-null    int64  
##  4   insulin   768 non-null    int64  
##  5   bmi       768 non-null    float64
##  6   pedigree  768 non-null    float64
##  7   age       768 non-null    int64  
##  8   label     768 non-null    int64  
## dtypes: float64(2), int64(7)
## memory usage: 54.1 KB</code></pre>
<div class="sourceCode" id="cb631"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb631-1"><a href="regularized-regression.html#cb631-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb631-2"><a href="regularized-regression.html#cb631-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> [<span class="op">*</span>a]</span>
<span id="cb631-3"><a href="regularized-regression.html#cb631-3" aria-hidden="true" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>## [1, 2, 3]</code></pre>
<div class="sourceCode" id="cb633"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb633-1"><a href="regularized-regression.html#cb633-1" aria-hidden="true" tabindex="-1"></a><span class="co">#split dataset in features and target variable</span></span>
<span id="cb633-2"><a href="regularized-regression.html#cb633-2" aria-hidden="true" tabindex="-1"></a>feature_cols <span class="op">=</span> [<span class="st">&#39;pregnant&#39;</span>, <span class="st">&#39;insulin&#39;</span>, <span class="st">&#39;bmi&#39;</span>, <span class="st">&#39;age&#39;</span>,<span class="st">&#39;glucose&#39;</span>,<span class="st">&#39;bp&#39;</span>,<span class="st">&#39;pedigree&#39;</span>]</span>
<span id="cb633-3"><a href="regularized-regression.html#cb633-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pima[feature_cols] <span class="co"># Features</span></span>
<span id="cb633-4"><a href="regularized-regression.html#cb633-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pima.label <span class="co"># Target variable</span></span>
<span id="cb633-5"><a href="regularized-regression.html#cb633-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb633-6"><a href="regularized-regression.html#cb633-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code></pre></div>
<div class="sourceCode" id="cb634"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb634-1"><a href="regularized-regression.html#cb634-1" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb634-2"><a href="regularized-regression.html#cb634-2" aria-hidden="true" tabindex="-1"></a>log_reg.fit(X_train,y_train)</span></code></pre></div>
<pre><code>## LogisticRegression(max_iter=200)</code></pre>
<div class="sourceCode" id="cb636"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb636-1"><a href="regularized-regression.html#cb636-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> log_reg.predict_proba(X_test)</span>
<span id="cb636-2"><a href="regularized-regression.html#cb636-2" aria-hidden="true" tabindex="-1"></a>y_pred_prob <span class="op">=</span> y_pred[:,<span class="dv">1</span>]</span></code></pre></div>
<div class="sourceCode" id="cb637"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb637-1"><a href="regularized-regression.html#cb637-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Setup arrays to store train and test accuracies</span></span>
<span id="cb637-2"><a href="regularized-regression.html#cb637-2" aria-hidden="true" tabindex="-1"></a><span class="co"># neighbors = np.arange(1, 20)</span></span>
<span id="cb637-3"><a href="regularized-regression.html#cb637-3" aria-hidden="true" tabindex="-1"></a><span class="co"># train_accuracy = np.empty(len(neighbors))</span></span>
<span id="cb637-4"><a href="regularized-regression.html#cb637-4" aria-hidden="true" tabindex="-1"></a><span class="co"># test_accuracy = np.empty(len(neighbors))</span></span>
<span id="cb637-5"><a href="regularized-regression.html#cb637-5" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb637-6"><a href="regularized-regression.html#cb637-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # Loop over different values of k</span></span>
<span id="cb637-7"><a href="regularized-regression.html#cb637-7" aria-hidden="true" tabindex="-1"></a><span class="co"># for i, k in enumerate(neighbors):</span></span>
<span id="cb637-8"><a href="regularized-regression.html#cb637-8" aria-hidden="true" tabindex="-1"></a><span class="co">#     # Setup a k-NN Classifier with k neighbors: knn</span></span>
<span id="cb637-9"><a href="regularized-regression.html#cb637-9" aria-hidden="true" tabindex="-1"></a><span class="co">#     knn = KNeighborsClassifier(___)</span></span>
<span id="cb637-10"><a href="regularized-regression.html#cb637-10" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb637-11"><a href="regularized-regression.html#cb637-11" aria-hidden="true" tabindex="-1"></a><span class="co">#     # Fit the classifier to the training data</span></span>
<span id="cb637-12"><a href="regularized-regression.html#cb637-12" aria-hidden="true" tabindex="-1"></a><span class="co">#     ___</span></span>
<span id="cb637-13"><a href="regularized-regression.html#cb637-13" aria-hidden="true" tabindex="-1"></a><span class="co">#     </span></span>
<span id="cb637-14"><a href="regularized-regression.html#cb637-14" aria-hidden="true" tabindex="-1"></a><span class="co">#     #Compute accuracy on the training set</span></span>
<span id="cb637-15"><a href="regularized-regression.html#cb637-15" aria-hidden="true" tabindex="-1"></a><span class="co">#     train_accuracy[i] = ___</span></span>
<span id="cb637-16"><a href="regularized-regression.html#cb637-16" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb637-17"><a href="regularized-regression.html#cb637-17" aria-hidden="true" tabindex="-1"></a><span class="co">#     #Compute accuracy on the testing set</span></span>
<span id="cb637-18"><a href="regularized-regression.html#cb637-18" aria-hidden="true" tabindex="-1"></a><span class="co">#     test_accuracy[i] = ___</span></span>
<span id="cb637-19"><a href="regularized-regression.html#cb637-19" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb637-20"><a href="regularized-regression.html#cb637-20" aria-hidden="true" tabindex="-1"></a><span class="co"># # Generate plot</span></span>
<span id="cb637-21"><a href="regularized-regression.html#cb637-21" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.title(&#39;k-NN: Varying Number of Neighbors&#39;)</span></span>
<span id="cb637-22"><a href="regularized-regression.html#cb637-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(___, label = &#39;Testing Accuracy&#39;)</span></span>
<span id="cb637-23"><a href="regularized-regression.html#cb637-23" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(___, label = &#39;Training Accuracy&#39;)</span></span>
<span id="cb637-24"><a href="regularized-regression.html#cb637-24" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.legend()</span></span>
<span id="cb637-25"><a href="regularized-regression.html#cb637-25" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.xlabel(&#39;Number of Neighbors&#39;)</span></span>
<span id="cb637-26"><a href="regularized-regression.html#cb637-26" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.ylabel(&#39;Accuracy&#39;)</span></span>
<span id="cb637-27"><a href="regularized-regression.html#cb637-27" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code></pre></div>
<div class="sourceCode" id="cb638"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb638-1"><a href="regularized-regression.html#cb638-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb638-2"><a href="regularized-regression.html#cb638-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb638-3"><a href="regularized-regression.html#cb638-3" aria-hidden="true" tabindex="-1"></a>fpr, tpr, thresholds <span class="op">=</span> roc_curve(y_test, y_pred_prob)</span>
<span id="cb638-4"><a href="regularized-regression.html#cb638-4" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">&#39;k--&#39;</span>)</span></code></pre></div>
<pre><code>## [&lt;matplotlib.lines.Line2D object at 0x7fb10b5cf080&gt;]</code></pre>
<div class="sourceCode" id="cb640"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb640-1"><a href="regularized-regression.html#cb640-1" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr, label<span class="op">=</span><span class="st">&#39;Logistic Regression&#39;</span>)</span></code></pre></div>
<pre><code>## [&lt;matplotlib.lines.Line2D object at 0x7fb10b5cf0f0&gt;]</code></pre>
<div class="sourceCode" id="cb642"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb642-1"><a href="regularized-regression.html#cb642-1" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;False Positive Rate&#39;</span>)</span></code></pre></div>
<pre><code>## Text(0.5, 0, &#39;False Positive Rate&#39;)</code></pre>
<div class="sourceCode" id="cb644"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb644-1"><a href="regularized-regression.html#cb644-1" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;True Positive Rate&#39;</span>)</span></code></pre></div>
<pre><code>## Text(0, 0.5, &#39;True Positive Rate&#39;)</code></pre>
<div class="sourceCode" id="cb646"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb646-1"><a href="regularized-regression.html#cb646-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Logistic Regression ROC Curve&#39;</span>)</span></code></pre></div>
<pre><code>## Text(0.5, 1.0, &#39;Logistic Regression ROC Curve&#39;)</code></pre>
<div class="sourceCode" id="cb648"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb648-1"><a href="regularized-regression.html#cb648-1" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code></pre></div>
<p><img src="BIPM_DS_files/figure-html/unnamed-chunk-238-1.png" width="672" /></p>
<p><strong>Area under the ROC curve (AUC)</strong></p>
<div class="sourceCode" id="cb649"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb649-1"><a href="regularized-regression.html#cb649-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb649-2"><a href="regularized-regression.html#cb649-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb649-3"><a href="regularized-regression.html#cb649-3" aria-hidden="true" tabindex="-1"></a>roc_auc_score(y_test, y_pred_prob)</span></code></pre></div>
<pre><code>## 0.7969370860927152</code></pre>
<p><strong>AUC using cross-validation</strong></p>
<div class="sourceCode" id="cb651"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb651-1"><a href="regularized-regression.html#cb651-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb651-2"><a href="regularized-regression.html#cb651-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb651-3"><a href="regularized-regression.html#cb651-3" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> cross_val_score(log_reg, X, y, cv<span class="op">=</span><span class="dv">10</span>, scoring<span class="op">=</span><span class="st">&#39;roc_auc&#39;</span>)</span>
<span id="cb651-4"><a href="regularized-regression.html#cb651-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb651-5"><a href="regularized-regression.html#cb651-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cv_scores)</span></code></pre></div>
<pre><code>## [0.789 0.83  0.838 0.773 0.812 0.848 0.819 0.921 0.857 0.833]</code></pre>
</div>
<div id="regularized-regression-1" class="section level3" number="9.1.4">
<h3><span class="header-section-number">9.1.4</span> Regularized Regression</h3>
<p>We will illustrate the concepts on the Boston housing data set</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb653-1"><a href="regularized-regression.html#cb653-1" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> pd.read_csv(<span class="st">&#39;../data/boston.csv&#39;</span>)</span>
<span id="cb653-2"><a href="regularized-regression.html#cb653-2" aria-hidden="true" tabindex="-1"></a><span class="co">#print(boston.head())</span></span>
<span id="cb653-3"><a href="regularized-regression.html#cb653-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> boston.drop(<span class="st">&#39;medv&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>).values</span>
<span id="cb653-4"><a href="regularized-regression.html#cb653-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> boston[<span class="st">&#39;medv&#39;</span>].values</span></code></pre></div>
<p><strong>Variable Selection</strong></p>
<p><a href="../figures/model_selection.pdf">ISLR slides on model selection</a></p>
<div id="l2-regression" class="section level4" number="9.1.4.1">
<h4><span class="header-section-number">9.1.4.1</span> L2 Regression</h4>
<p>Recall: Linear regression minimizes a loss function
- It chooses a coefficient for each feature variable
- Large coefficients can lead to overfitting
- Penalizing large coefficients: Regularization</p>
<p><strong>Detour: <span class="math inline">\(L_p\)</span> norms</strong></p>
<p><a href="http://mathworld.wolfram.com/VectorNorm.html" class="uri">http://mathworld.wolfram.com/VectorNorm.html</a></p>
<p>Our new penalty term in finding the coefficients <span class="math inline">\(\beta_j\)</span> is the minimization</p>
<p><span class="math display">\[
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_j x_{ij}} \right)^2} + \lambda \sum_{j=1}^p{\beta_j^2} = RSS + \lambda \sum_{j=1}^p{\beta_j^2}
\]</span></p>
<p>Instead of obtaining <strong>one</strong> set of coefficients we have a dependency of <span class="math inline">\(\beta_j\)</span> on <span class="math inline">\(\lambda\)</span>:</p>
<p><img src="../figures/RidgeCoefficients1.png" width=600></p>
<div class="sourceCode" id="cb654"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb654-1"><a href="regularized-regression.html#cb654-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb654-2"><a href="regularized-regression.html#cb654-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y,test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb654-3"><a href="regularized-regression.html#cb654-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb654-4"><a href="regularized-regression.html#cb654-4" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>, normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb654-5"><a href="regularized-regression.html#cb654-5" aria-hidden="true" tabindex="-1"></a>ridge.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## Ridge(alpha=0.1, normalize=True)</code></pre>
<div class="sourceCode" id="cb656"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb656-1"><a href="regularized-regression.html#cb656-1" aria-hidden="true" tabindex="-1"></a>ridge_pred <span class="op">=</span> ridge.predict(X_test)<span class="co">#?does this automatically take care of nromalization ??</span></span>
<span id="cb656-2"><a href="regularized-regression.html#cb656-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Returns the coefficient of determination R^2 of the prediction.</span></span>
<span id="cb656-3"><a href="regularized-regression.html#cb656-3" aria-hidden="true" tabindex="-1"></a>ridge.score(X_test, y_test)</span></code></pre></div>
<pre><code>## 0.6996938275127313</code></pre>
<div class="sourceCode" id="cb658"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb658-1"><a href="regularized-regression.html#cb658-1" aria-hidden="true" tabindex="-1"></a>ridge_pred[<span class="dv">0</span>:<span class="dv">5</span>]</span></code></pre></div>
<pre><code>## array([27.97 , 35.383, 16.826, 25.145, 18.749])</code></pre>
<div class="sourceCode" id="cb660"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb660-1"><a href="regularized-regression.html#cb660-1" aria-hidden="true" tabindex="-1"></a>ridge2 <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>, normalize<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb660-2"><a href="regularized-regression.html#cb660-2" aria-hidden="true" tabindex="-1"></a>ridge2.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## Ridge(alpha=0.1)</code></pre>
<div class="sourceCode" id="cb662"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb662-1"><a href="regularized-regression.html#cb662-1" aria-hidden="true" tabindex="-1"></a>ridge2_pred <span class="op">=</span> ridge2.predict(X_test)</span>
<span id="cb662-2"><a href="regularized-regression.html#cb662-2" aria-hidden="true" tabindex="-1"></a>ridge2_pred[<span class="dv">0</span>:<span class="dv">5</span>]</span></code></pre></div>
<pre><code>## array([28.593, 36.453, 15.304, 25.371, 18.912])</code></pre>
<div class="sourceCode" id="cb664"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb664-1"><a href="regularized-regression.html#cb664-1" aria-hidden="true" tabindex="-1"></a>ridge2.coef_</span></code></pre></div>
<pre><code>## array([-1.325e-01,  3.599e-02,  4.385e-02,  3.096e+00, -1.406e+01,
##         4.061e+00, -1.200e-02, -1.365e+00,  2.395e-01, -8.817e-03,
##        -8.955e-01,  1.183e-02, -5.498e-01])</code></pre>
</div>
<div id="l1-regression" class="section level4" number="9.1.4.2">
<h4><span class="header-section-number">9.1.4.2</span> L1 Regression</h4>
<p>Our penalty termy looks slightly different (with big consequences for <strong>sparsity</strong>)</p>
<p><span class="math display">\[
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_j x_{ij}} \right)^2} + \lambda \sum_{j=1}^p{| \beta_j |} = RSS + \lambda \sum_{j=1}^p{| \beta_j |}
\]</span></p>
<p><img src="../figures/LassoCoefficients1.png" width=600></p>
<p>(Comment: <em>LASSO</em> = ‚Äúleast absolute shrinkage and selection operator‚Äù)</p>
<div class="sourceCode" id="cb666"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb666-1"><a href="regularized-regression.html#cb666-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb666-2"><a href="regularized-regression.html#cb666-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb666-3"><a href="regularized-regression.html#cb666-3" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>, normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb666-4"><a href="regularized-regression.html#cb666-4" aria-hidden="true" tabindex="-1"></a>lasso.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## Lasso(alpha=0.1, normalize=True)</code></pre>
<div class="sourceCode" id="cb668"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb668-1"><a href="regularized-regression.html#cb668-1" aria-hidden="true" tabindex="-1"></a>lasso_pred <span class="op">=</span> lasso.predict(X_test)</span>
<span id="cb668-2"><a href="regularized-regression.html#cb668-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Returns the coefficient of determination R^2 of the prediction.</span></span>
<span id="cb668-3"><a href="regularized-regression.html#cb668-3" aria-hidden="true" tabindex="-1"></a>lasso.score(X_test, y_test)</span></code></pre></div>
<pre><code>## 0.5950229535328551</code></pre>
<div class="sourceCode" id="cb670"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb670-1"><a href="regularized-regression.html#cb670-1" aria-hidden="true" tabindex="-1"></a>lasso.coef_</span></code></pre></div>
<pre><code>## array([-0.   ,  0.   , -0.   ,  0.   , -0.   ,  3.189, -0.   , -0.   ,
##        -0.   , -0.   , -0.307,  0.   , -0.487])</code></pre>
<p><strong>Feature Selection Property of the LASSO</strong></p>
<div class="sourceCode" id="cb672"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb672-1"><a href="regularized-regression.html#cb672-1" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> boston.drop(<span class="st">&#39;medv&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>).columns</span>
<span id="cb672-2"><a href="regularized-regression.html#cb672-2" aria-hidden="true" tabindex="-1"></a>lasso_coef <span class="op">=</span> lasso.fit(X, y).coef_</span>
<span id="cb672-3"><a href="regularized-regression.html#cb672-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.plot(<span class="bu">range</span>(<span class="bu">len</span>(names)), lasso_coef)</span>
<span id="cb672-4"><a href="regularized-regression.html#cb672-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.xticks(<span class="bu">range</span>(<span class="bu">len</span>(names)), names, rotation<span class="op">=</span><span class="dv">60</span>)</span>
<span id="cb672-5"><a href="regularized-regression.html#cb672-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.ylabel(<span class="st">&#39;Coefficients&#39;</span>)</span>
<span id="cb672-6"><a href="regularized-regression.html#cb672-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="BIPM_DS_files/figure-html/unnamed-chunk-245-1.png" width="672" /></p>
<p><strong>Tuning the shrinkage parameter with CV</strong></p>
<p>(It seems that <span class="math inline">\(\lambda\)</span> is often referred to as <span class="math inline">\(\alpha\)</span>)</p>
<p>From sklearn.linear_model:</p>
<ul>
<li>LassoCV</li>
<li>RidgeCV</li>
<li>GridSearchCV</li>
</ul>
<div class="sourceCode" id="cb673"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb673-1"><a href="regularized-regression.html#cb673-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> RidgeCV</span>
<span id="cb673-2"><a href="regularized-regression.html#cb673-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb673-3"><a href="regularized-regression.html#cb673-3" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> RidgeCV(alphas<span class="op">=</span>[<span class="fl">1e-3</span>, <span class="fl">1e-2</span>, <span class="fl">1e-1</span>, <span class="dv">1</span>],store_cv_values <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb673-4"><a href="regularized-regression.html#cb673-4" aria-hidden="true" tabindex="-1"></a>reg.fit(X, y)</span></code></pre></div>
<pre><code>## RidgeCV(alphas=array([0.001, 0.01 , 0.1  , 1.   ]), store_cv_values=True)</code></pre>
<div class="sourceCode" id="cb675"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb675-1"><a href="regularized-regression.html#cb675-1" aria-hidden="true" tabindex="-1"></a>reg.score(X, y)</span></code></pre></div>
<pre><code>## 0.7406421899247916</code></pre>
<div class="sourceCode" id="cb677"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb677-1"><a href="regularized-regression.html#cb677-1" aria-hidden="true" tabindex="-1"></a>reg.cv_values_</span>
<span id="cb677-2"><a href="regularized-regression.html#cb677-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Open Questions:</span></span>
<span id="cb677-3"><a href="regularized-regression.html#cb677-3" aria-hidden="true" tabindex="-1"></a><span class="co">#1. how to extract all scores, possibly even the individual folds?</span></span>
<span id="cb677-4"><a href="regularized-regression.html#cb677-4" aria-hidden="true" tabindex="-1"></a><span class="co">#2. What is the optimal alpha ??</span></span>
<span id="cb677-5"><a href="regularized-regression.html#cb677-5" aria-hidden="true" tabindex="-1"></a><span class="co">#reg.cv_values_</span></span></code></pre></div>
<pre><code>## array([[ 37.303,  37.347,  37.766,  40.434],
##        [ 11.997,  11.974,  11.757,  10.501],
##        [ 17.488,  17.492,  17.532,  17.786],
##        ...,
##        [ 14.642,  14.676,  15.003,  17.073],
##        [ 17.862,  17.902,  18.281,  20.683],
##        [113.709, 113.813, 114.793, 120.997]])</code></pre>
<div class="sourceCode" id="cb679"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb679-1"><a href="regularized-regression.html#cb679-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LassoCV</span>
<span id="cb679-2"><a href="regularized-regression.html#cb679-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb679-3"><a href="regularized-regression.html#cb679-3" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LassoCV(cv<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">0</span>).fit(X, y)</span>
<span id="cb679-4"><a href="regularized-regression.html#cb679-4" aria-hidden="true" tabindex="-1"></a>reg.score(X, y)</span></code></pre></div>
<pre><code>## 0.7024437179872696</code></pre>
<div class="sourceCode" id="cb681"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb681-1"><a href="regularized-regression.html#cb681-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb681-2"><a href="regularized-regression.html#cb681-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb681-3"><a href="regularized-regression.html#cb681-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb681-4"><a href="regularized-regression.html#cb681-4" aria-hidden="true" tabindex="-1"></a>diabetes <span class="op">=</span> datasets.load_diabetes()</span>
<span id="cb681-5"><a href="regularized-regression.html#cb681-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> diabetes.data[:<span class="dv">150</span>]</span>
<span id="cb681-6"><a href="regularized-regression.html#cb681-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> diabetes.target[:<span class="dv">150</span>]</span>
<span id="cb681-7"><a href="regularized-regression.html#cb681-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb681-8"><a href="regularized-regression.html#cb681-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb681-9"><a href="regularized-regression.html#cb681-9" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(random_state<span class="op">=</span><span class="dv">0</span>,max_iter<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb681-10"><a href="regularized-regression.html#cb681-10" aria-hidden="true" tabindex="-1"></a><span class="co">#logarithmically spaced sequence</span></span>
<span id="cb681-11"><a href="regularized-regression.html#cb681-11" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="dv">20</span>)</span>
<span id="cb681-12"><a href="regularized-regression.html#cb681-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb681-13"><a href="regularized-regression.html#cb681-13" aria-hidden="true" tabindex="-1"></a>tuned_parameters <span class="op">=</span> [{<span class="st">&#39;alpha&#39;</span>: alphas}]</span>
<span id="cb681-14"><a href="regularized-regression.html#cb681-14" aria-hidden="true" tabindex="-1"></a>n_folds <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb681-15"><a href="regularized-regression.html#cb681-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb681-16"><a href="regularized-regression.html#cb681-16" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> GridSearchCV(lasso, tuned_parameters, cv<span class="op">=</span>n_folds, refit<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb681-17"><a href="regularized-regression.html#cb681-17" aria-hidden="true" tabindex="-1"></a>reg.fit(X, y)</span></code></pre></div>
<pre><code>## GridSearchCV(cv=5, estimator=Lasso(max_iter=2000, random_state=0),
##              param_grid=[{&#39;alpha&#39;: array([1.000e-04, 1.528e-04, 2.336e-04, 3.570e-04, 5.456e-04, 8.338e-04,
##        1.274e-03, 1.947e-03, 2.976e-03, 4.549e-03, 6.952e-03, 1.062e-02,
##        1.624e-02, 2.482e-02, 3.793e-02, 5.796e-02, 8.859e-02, 1.354e-01,
##        2.069e-01, 3.162e-01])}],
##              refit=False)</code></pre>
<div class="sourceCode" id="cb683"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb683-1"><a href="regularized-regression.html#cb683-1" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> reg.cv_results_[<span class="st">&#39;mean_test_score&#39;</span>]</span>
<span id="cb683-2"><a href="regularized-regression.html#cb683-2" aria-hidden="true" tabindex="-1"></a>scores_std <span class="op">=</span> reg.cv_results_[<span class="st">&#39;std_test_score&#39;</span>]</span>
<span id="cb683-3"><a href="regularized-regression.html#cb683-3" aria-hidden="true" tabindex="-1"></a>plt.figure().set_size_inches(<span class="dv">8</span>, <span class="dv">6</span>)</span>
<span id="cb683-4"><a href="regularized-regression.html#cb683-4" aria-hidden="true" tabindex="-1"></a>plt.semilogx(alphas, scores)</span>
<span id="cb683-5"><a href="regularized-regression.html#cb683-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb683-6"><a href="regularized-regression.html#cb683-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot error lines showing +/- std. errors of the scores</span></span></code></pre></div>
<pre><code>## [&lt;matplotlib.lines.Line2D object at 0x7fb109c47dd8&gt;]</code></pre>
<div class="sourceCode" id="cb685"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb685-1"><a href="regularized-regression.html#cb685-1" aria-hidden="true" tabindex="-1"></a>std_error <span class="op">=</span> scores_std <span class="op">/</span> np.sqrt(n_folds)</span>
<span id="cb685-2"><a href="regularized-regression.html#cb685-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb685-3"><a href="regularized-regression.html#cb685-3" aria-hidden="true" tabindex="-1"></a>plt.semilogx(alphas, scores <span class="op">+</span> std_error, <span class="st">&#39;b--&#39;</span>)</span></code></pre></div>
<pre><code>## [&lt;matplotlib.lines.Line2D object at 0x7fb109c705f8&gt;]</code></pre>
<div class="sourceCode" id="cb687"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb687-1"><a href="regularized-regression.html#cb687-1" aria-hidden="true" tabindex="-1"></a>plt.semilogx(alphas, scores <span class="op">-</span> std_error, <span class="st">&#39;b--&#39;</span>)</span>
<span id="cb687-2"><a href="regularized-regression.html#cb687-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb687-3"><a href="regularized-regression.html#cb687-3" aria-hidden="true" tabindex="-1"></a><span class="co"># alpha=0.2 controls the translucency of the fill color</span></span></code></pre></div>
<pre><code>## [&lt;matplotlib.lines.Line2D object at 0x7fb10b842710&gt;]</code></pre>
<div class="sourceCode" id="cb689"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb689-1"><a href="regularized-regression.html#cb689-1" aria-hidden="true" tabindex="-1"></a>plt.fill_between(alphas, scores <span class="op">+</span> std_error, scores <span class="op">-</span> std_error, alpha<span class="op">=</span><span class="fl">0.2</span>)</span></code></pre></div>
<pre><code>## &lt;matplotlib.collections.PolyCollection object at 0x7fb10b87f588&gt;</code></pre>
<div class="sourceCode" id="cb691"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb691-1"><a href="regularized-regression.html#cb691-1" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;CV score +/- std error&#39;</span>)</span></code></pre></div>
<pre><code>## Text(0, 0.5, &#39;CV score +/- std error&#39;)</code></pre>
<div class="sourceCode" id="cb693"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb693-1"><a href="regularized-regression.html#cb693-1" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;alpha&#39;</span>)</span></code></pre></div>
<pre><code>## Text(0.5, 0, &#39;alpha&#39;)</code></pre>
<div class="sourceCode" id="cb695"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb695-1"><a href="regularized-regression.html#cb695-1" aria-hidden="true" tabindex="-1"></a>plt.axhline(np.<span class="bu">max</span>(scores), linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, color<span class="op">=</span><span class="st">&#39;.5&#39;</span>)</span></code></pre></div>
<pre><code>## &lt;matplotlib.lines.Line2D object at 0x7fb10b87f828&gt;</code></pre>
<div class="sourceCode" id="cb697"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb697-1"><a href="regularized-regression.html#cb697-1" aria-hidden="true" tabindex="-1"></a>plt.xlim([alphas[<span class="dv">0</span>], alphas[<span class="op">-</span><span class="dv">1</span>]])</span></code></pre></div>
<pre><code>## (0.0001, 0.31622776601683794)</code></pre>
<p><strong>How much can you trust the selection of alpha?</strong></p>
<p>Task: Find the opimal Alpha parameters (maximising the generalization score) on different subsets of the data</p>
<!-- #region -->
</div>
<div id="elasticnet" class="section level4" number="9.1.4.3">
<h4><span class="header-section-number">9.1.4.3</span> ElasticNet</h4>
<p><strong>The mystery of the additional <span class="math inline">\(\alpha\)</span> paramater</strong></p>
<ul>
<li>The elastic net for correlated variables, which uses a penalty that is part L1, part L2.</li>
<li>Compromise between the ridge regression penalty <span class="math inline">\((\alpha = 0)\)</span> and the lasso penalty <span class="math inline">\((\alpha = 1)\)</span>.</li>
<li>This penalty is particularly useful in the <span class="math inline">\(p &gt;&gt; N\)</span> situation, or any situation where there are many correlated predictor variables.</li>
</ul>
<p><span class="math display">\[
 RSS + \lambda \sum_{j=1}^p{ \left( \frac{1}{2} (1-\alpha) \beta_j^2 + \alpha | \beta_j | \right)}
\]</span>
The right hand side can be written as
<span class="math display">\[
  \sum_{j=1}^p{ \frac{1}{2} \lambda (1-\alpha) \beta_j^2 + \alpha \lambda | \beta_j |} = \sum_{j=1}^p{  \lambda_R \beta_j^2 +  \lambda_L | \beta_j |}
\]</span>
with the Ridge penalty parameter <span class="math inline">\(\lambda_R \equiv \frac{1}{2} \lambda (1-\alpha)\)</span> and the lasso penalty parameter <span class="math inline">\(\lambda_L \equiv \alpha \lambda\)</span>.
So we see that with
<span class="math display">\[
\alpha = \frac{\lambda_L}{\lambda_L+ 2 \lambda_R}, \mbox{ and } \lambda= \lambda_L+ 2 \lambda_R
\]</span>
<!-- #endregion --></p>
<p>Further Reading:
- this <a href="plot_lasso_coordinate_descent_path.ipynb">notebook</a> shows how to plot the entire ‚Äúpath‚Äù of coefficients.</p>
</div>
</div>
<div id="kaggle" class="section level3" number="9.1.5">
<h3><span class="header-section-number">9.1.5</span> Kaggle</h3>
<div id="housing-data" class="section level4 unlisted unnumbered">
<h4><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">Housing Data</a></h4>
<p>This <a href="../data/kaggle/HousePrices/EDA.ipynb">notebook</a> (despite its annoying ‚Äúhumor‚Äù) is a good start.
(Get it <a href="https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python">directly</a> from kaggle)</p>
</div>
<div id="your-first-submission" class="section level4 unlisted unnumbered">
<h4>Your first submission</h4>
<p><img src="../figures/kaggle_HousePrices1.jpg" width=600></p>
<div class="sourceCode" id="cb699"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb699-1"><a href="regularized-regression.html#cb699-1" aria-hidden="true" tabindex="-1"></a>df_train <span class="op">=</span> pd.read_csv(<span class="st">&#39;../data/kaggle/HousePrices/train.csv&#39;</span>)</span>
<span id="cb699-2"><a href="regularized-regression.html#cb699-2" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> pd.read_csv(<span class="st">&#39;../data/kaggle/HousePrices/test.csv&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb700"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb700-1"><a href="regularized-regression.html#cb700-1" aria-hidden="true" tabindex="-1"></a>df_train</span></code></pre></div>
<pre><code>##         Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice
## 0        1          60       RL  ...        WD         Normal    208500
## 1        2          20       RL  ...        WD         Normal    181500
## 2        3          60       RL  ...        WD         Normal    223500
## 3        4          70       RL  ...        WD        Abnorml    140000
## 4        5          60       RL  ...        WD         Normal    250000
## ...    ...         ...      ...  ...       ...            ...       ...
## 1455  1456          60       RL  ...        WD         Normal    175000
## 1456  1457          20       RL  ...        WD         Normal    210000
## 1457  1458          70       RL  ...        WD         Normal    266500
## 1458  1459          20       RL  ...        WD         Normal    142125
## 1459  1460          20       RL  ...        WD         Normal    147500
## 
## [1460 rows x 81 columns]</code></pre>
<div class="sourceCode" id="cb702"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb702-1"><a href="regularized-regression.html#cb702-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Submission:</span></span>
<span id="cb702-2"><a href="regularized-regression.html#cb702-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pred_df = pd.DataFrame(y_pred, index=df_test.index, columns=[&quot;SalePrice&quot;])</span></span>
<span id="cb702-3"><a href="regularized-regression.html#cb702-3" aria-hidden="true" tabindex="-1"></a><span class="co"># pred_df.to_csv(&#39;../data/kaggle/HousePrices/submissions/en1.csv&#39;, header=True, index_label=&#39;Id&#39;)</span></span></code></pre></div>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["BIPM_DS.pdf", "BIPM_DS.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
