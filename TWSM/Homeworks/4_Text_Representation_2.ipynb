{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EijbeBX0qx_H"
   },
   "source": [
    "# Exercise 4. Text Representation Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toZg6eERq72W"
   },
   "source": [
    "In this exercise we will apply the following models to the stemmed data from Exercise 2:\n",
    "\n",
    "1.   Word2Vec\n",
    "2.   Doc2vec\n",
    "3.   BERT\n",
    "\n",
    "At the end, we will derive a corpus with each of them which can be used in downstream tasks such as classification and clustering (see next exercises).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9ja1Ri6c1lG",
    "outputId": "7aed9153-dffc-42b3-a036-9a8fda85120b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 5.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 18.6MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 36.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qPiuxK5jrVM4"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk2E1gjxrXxj"
   },
   "source": [
    "## 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKLv-1vX-mRI",
    "outputId": "2a6f918a-fe5f-48b5-930a-7efb732ddc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKnjwa_mu-hN",
    "outputId": "c0f88262-b418-47b6-afd8-310b6097944d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine specs year production car history info funky looking car mail thank\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "data_lemma=pickle.load(open(\"/content/drive/MyDrive/TWSM_Data/Lemma.pkl\", \"rb\"))\n",
    "print(data_lemma[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C8XaaSrrb6T"
   },
   "source": [
    "## 1. Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdV3nVDJaYkc"
   },
   "source": [
    "In this section we will train the word2vec model on the lemmatized data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-CmhI73QwURO"
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset for the word2vec model\n",
    "corpus_gen=[doc.split() for doc in data_lemma]\n",
    "\n",
    "# Train the model for embeddings of size 100 considering words appearing in more than 566 documents, default window=5\n",
    "model = Word2Vec(corpus_gen, size=100, min_count=566)\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkOJS7nV216t",
    "outputId": "0bfd841d-cd27-416d-9519-6eff16c87eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'accept', 'access', 'act', 'action', 'actually', 'add', 'address', 'advance', 'ago', 'agree', 'air', 'allow', 'american', 'answer', 'anti', 'anybody', 'appear', 'apple', 'application', 'apply', 'appreciate', 'apr', 'april', 'area', 'argument', 'arm', 'armenian', 'armenians', 'article', 'ask', 'assume', 'atheist', 'attack', 'attempt', 'available', 'away', 'bad', 'base', 'bear', 'begin', 'belief', 'believe', 'better', 'bible', 'big', 'bike', 'bit', 'black', 'board', 'body', 'book', 'box', 'break', 'bring', 'build', 'bus', 'business', 'buy', 'call', 'canada', 'car', 'card', 'care', 'carry', 'case', 'cause', 'center', 'certain', 'certainly', 'change', 'cheap', 'check', 'child', 'chip', 'choose', 'christ', 'christian', 'christians', 'church', 'city', 'claim', 'clear', 'clinton', 'clipper', 'close', 'code', 'color', 'com', 'come', 'command', 'comment', 'common', 'company', 'condition', 'consider', 'contact', 'contain', 'continue', 'control', 'copy', 'correct', 'cost', 'country', 'couple', 'course', 'cover', 'create', 'crime', 'current', 'cut', 'data', 'date', 'datum', 'david', 'day', 'dead', 'deal', 'death', 'decide', 'define', 'design', 'device', 'die', 'difference', 'different', 'disclaimer', 'discussion', 'disk', 'display', 'do', 'dod', 'dos', 'drive', 'driver', 'drug', 'early', 'earth', 'easy', 'edu', 'effect', 'email', 'encryption', 'end', 'entry', 'error', 'especially', 'event', 'evidence', 'exactly', 'example', 'exist', 'expect', 'experience', 'explain', 'face', 'fact', 'fail', 'faith', 'fall', 'fan', 'faq', 'far', 'fast', 'fax', 'feel', 'figure', 'file', 'fine', 'flame', 'follow', 'food', 'force', 'form', 'format', 'free', 'friend', 'ftp', 'function', 'game', 'general', 'get', 'give', 'go', 'goal', 'god', 'good', 'gov', 'government', 'graphic', 'great', 'ground', 'group', 'guess', 'gun', 'guy', 'hand', 'happen', 'hard', 'hardware', 'have', 'haven', 'head', 'health', 'hear', 'hell', 'help', 'high', 'history', 'hit', 'hockey', 'hold', 'home', 'hope', 'house', 'human', 'ibm', 'idea', 'image', 'important', 'include', 'increase', 'individual', 'info', 'information', 'instead', 'interested', 'internet', 'involve', 'isn', 'israel', 'israeli', 'issue', 'jesus', 'jewish', 'jews', 'jim', 'job', 'john', 'keep', 'key', 'kill', 'kind', 'know', 'land', 'large', 'late', 'later', 'launch', 'law', 'lead', 'league', 'learn', 'leave', 'let', 'level', 'lie', 'life', 'light', 'like', 'likely', 'line', 'list', 'little', 'live', 'local', 'long', 'look', 'lose', 'lot', 'love', 'low', 'mac', 'machine', 'mail', 'major', 'make', 'man', 'manager', 'mark', 'matter', 'max', 'maybe', 'mean', 'member', 'memory', 'mention', 'message', 'michael', 'mike', 'million', 'mind', 'mit', 'mode', 'model', 'money', 'monitor', 'month', 'mouse', 'nasa', 'national', 'need', 'net', 'netcom', 'network', 'new', 'news', 'newsgroup', 'nice', 'night', 'non', 'note', 'number', 'offer', 'office', 'old', 'open', 'opinion', 'order', 'org', 'original', 'output', 'package', 'page', 'particular', 'pass', 'paul', 'pay', 'people', 'period', 'person', 'phone', 'pick', 'place', 'plan', 'play', 'player', 'point', 'police', 'policy', 'political', 'port', 'position', 'possible', 'post', 'power', 'present', 'president', 'press', 'pretty', 'price', 'private', 'probably', 'problem', 'process', 'produce', 'product', 'program', 'project', 'protect', 'prove', 'provide', 'pub', 'public', 'purpose', 'question', 'quote', 'radio', 'rate', 'read', 'real', 'reason', 'receive', 'record', 'red', 'reference', 'release', 'religion', 'remember', 'reply', 'report', 'request', 'require', 'research', 'response', 'rest', 'result', 'return', 'right', 'road', 'rule', 'run', 'sale', 'save', 'say', 'school', 'science', 'screen', 'scsi', 'season', 'second', 'section', 'security', 'see', 'self', 'sell', 'send', 'sense', 'server', 'service', 'set', 'short', 'show', 'similar', 'simple', 'simply', 'single', 'site', 'situation', 'size', 'small', 'software', 'sorry', 'sort', 'sound', 'source', 'space', 'speak', 'speed', 'stand', 'standard', 'start', 'state', 'statement', 'steve', 'stop', 'story', 'strong', 'study', 'stuff', 'subject', 'suggest', 'sun', 'support', 'suppose', 'sure', 'system', 'take', 'talk', 'tape', 'team', 'technology', 'tell', 'term', 'test', 'text', 'thank', 'thing', 'think', 'time', 'today', 'true', 'truth', 'try', 'turkish', 'turn', 'type', 'uiuc', 'understand', 'university', 'unix', 'use', 'user', 'usually', 'uucp', 'value', 'version', 'video', 'view', 'wait', 'want', 'war', 'washington', 'watch', 'way', 'weapon', 'week', 'well', 'white', 'widget', 'win', 'window', 'windows', 'woman', 'wonder', 'word', 'work', 'world', 'wouldn', 'write', 'wrong', 'year', 'yes', 'young']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in sorted(model.wv.vocab.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T61g8-b1xN6k",
    "outputId": "20f74fb5-dfb1-41ab-8272-8adae4442154"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4692358 , -0.8023282 , -0.13441241,  0.98153615,  0.7599832 ,\n",
       "       -0.40347973,  1.0198705 , -1.5354207 , -0.9257218 , -1.0203286 ,\n",
       "        2.215068  ,  0.6632183 , -0.18657741, -1.9154484 ,  0.44888872,\n",
       "       -0.02129542,  0.798015  ,  0.37241736, -0.9987019 , -0.11381808,\n",
       "        1.187318  ,  0.87888634,  0.7570844 , -1.2267259 ,  0.18649325,\n",
       "       -0.83251554,  0.41996965,  0.52846485, -0.25219265,  0.4602982 ,\n",
       "        0.49639538, -0.14283605, -0.30835825, -0.32588053,  0.39061022,\n",
       "       -0.6104033 ,  0.7260111 ,  0.17973503, -0.89412355, -0.06654264,\n",
       "       -0.2113204 ,  0.61939704, -1.3517871 , -1.120821  , -1.8280628 ,\n",
       "       -0.1121858 , -0.88791466, -1.8047341 , -0.11037601,  1.8896706 ,\n",
       "        0.69367045, -0.6998514 , -1.2943509 , -0.42140684,  0.11713248,\n",
       "        1.9989212 ,  1.23207   , -0.23176973,  0.6543441 ,  0.05240316,\n",
       "        0.2936794 , -0.62970936, -1.8247054 ,  0.63928753, -0.51164234,\n",
       "        0.20888922, -1.4701684 ,  0.9333078 , -0.9873637 ,  0.15584892,\n",
       "        0.9281967 ,  1.107992  ,  0.4098628 ,  0.48646018,  0.64660126,\n",
       "       -0.41505376,  1.2091446 ,  0.20383301, -0.8500132 ,  0.42666826,\n",
       "       -1.0381694 ,  0.8872455 ,  0.43795434, -0.13272214, -0.19978745,\n",
       "       -0.2808873 , -0.22016817,  0.15674557, -0.30307466, -0.79591966,\n",
       "        0.93527824, -1.3076534 ,  0.99318004,  1.4528227 , -1.5850452 ,\n",
       "        0.0190071 , -0.44637957,  1.397578  , -0.20309398, -1.3044388 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding for 'car'\n",
    "vector = model.wv['car']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXNcOMaEy_w7",
    "outputId": "85c44128-38ee-45ef-ae84-d9522b245917"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bike', 0.5988668203353882),\n",
       " ('buy', 0.5692394971847534),\n",
       " ('friend', 0.5084896087646484),\n",
       " ('get', 0.4868040978908539),\n",
       " ('light', 0.4818478226661682),\n",
       " ('guy', 0.44782453775405884),\n",
       " ('sell', 0.44778281450271606),\n",
       " ('hit', 0.44717222452163696),\n",
       " ('price', 0.44604820013046265),\n",
       " ('figure', 0.4446602463722229)]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most similar representations to 'car' based on cosine similarity\n",
    "model.wv.most_similar('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYduazP05lH-",
    "outputId": "b28a1554-086d-4d2e-cee7-4f22f10bb048"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fast', 0.6370933651924133)]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings' arithmetics\n",
    "model.wv.most_similar(positive=['bike', 'machine'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8OM3aCWvEZC"
   },
   "source": [
    "In the following we will derive the corpus. Note that word2vec (as opposed to doc2vec) generates one embedding for each word in the document. These then need to be aggregated at a document level. The simplest way is to determine the average over all words, but you can also use other aggregators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFBqJ2VS2rYL",
    "outputId": "3d386247-e43e-4113-86b5-81c21084556c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34941855  0.01060302 -0.13883016  0.22131062  0.1513066   0.19923933\n",
      " -0.20267347  0.27049688  0.1731505  -0.18897456  0.17501701 -0.14778358\n",
      "  0.21394292 -0.4481746   0.41000423 -0.04977489  0.46202007  0.13924809\n",
      " -0.37535664 -0.5306008   0.16376682 -0.21332453  0.11499792 -0.38966808\n",
      " -0.03403829  0.12516156 -0.01973565  0.1739679   0.1325702   0.22709534\n",
      "  0.173799   -0.10952532 -0.1093619   0.22009818 -0.34225252  0.2931799\n",
      "  0.31072828 -0.25662416 -0.10087096 -0.06631611 -0.09795035 -0.12415282\n",
      "  0.273179   -0.3497715  -0.18196814 -0.03896669 -0.16703357 -0.12678465\n",
      " -0.27670532  0.34497356  0.1883907   0.17308103 -0.65508145 -0.2483243\n",
      "  0.15495154  0.39163736  0.36049613  0.5655754   0.13507493  0.1137229\n",
      " -0.06360883  0.29676926 -0.38424855  0.18419804 -0.42886284 -0.09269961\n",
      " -0.12366384  0.47696677 -0.03065995  0.21037775  0.20514514 -0.09056196\n",
      " -0.04758799 -0.04493897  0.27887338 -0.1254085   0.01660353 -0.6270879\n",
      "  0.08722655  0.5494137  -0.11530174  0.14992364  0.17197424  0.17805006\n",
      " -0.06631165  0.27178627  0.10503238  0.25359243  0.49728304 -0.03231942\n",
      "  0.1083274  -0.18932337  0.24180368  0.10738519 -0.5646828   0.15991618\n",
      "  0.04761404  0.3310655   0.47750077 -0.32257617]\n"
     ]
    }
   ],
   "source": [
    "# Document representation for the text\n",
    "corpus_w2v=[[model.wv[word] for word in doc if word in model.wv.vocab.keys()] for doc in corpus_gen]\n",
    "positive=[i for i in range(len(corpus_gen)) if len(corpus_w2v[i])>0]\n",
    "\n",
    "corpus_w2v2=[corpus_w2v[i] for i in positive]\n",
    "data_lemma2=[data_lemma[i] for i in positive]\n",
    "\n",
    "# Document average representation\n",
    "corpus_w2v_avg_clean=[sum(words)/len(words) for words in corpus_w2v2]\n",
    "\n",
    "# This corpus can be used later in clustering and classification tasks\n",
    "print(corpus_w2v_avg_clean[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKUP2dEI-y51",
    "outputId": "6fe043eb-23f9-4abe-8084-2726d6ff61d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3p2H3WBAAdLk",
    "outputId": "2e0cdf30-a984-4a17-f410-e345f2540ec9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11298"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_w2v_avg_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olxITXGWBBw8",
    "outputId": "d25e490e-cd67-4599-ec3d-5438e590ce8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11298"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lemma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdDZTUaCVx7g",
    "outputId": "5d49f9ca-2c27-4347-971e-60aca6b7fca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'friend', 'buy', 'see', 'bike', 'get', 'look', 'month', 'lot', 'remember']\n",
      "car wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine specs year production car history info funky looking car mail thank\n",
      "\n",
      "aussie need info car show australia car enthusiast australia particularly interested american muscle car make amc ford chrysler mopar usa weeks june chicago sun thursday denver friday sunday austin texas monday friday oklahoma city friday monday anaheim california tuesday thursday las vegas nevada friday sunday grand canion monday tuesday june las angeles san diego vicinity wednesday june sunday june june south lake tahoe cal sunday june wednesday june reno thursday june san fransisco thursday june sunday june wonder send information car show swap meets drag meet model car show period anybody tell pomona swap meet year place visit car museum private collection collection bit information appreciate interested find model car scale model interste amc car particular amx javelin scrambler rebel machine kit plastic diecast interested selling tell interested send bring model australian high performance car interest reply email johnt spri level unisa edu thanks john tsimbinos\n"
     ]
    }
   ],
   "source": [
    "# Most simlar words to the document based on average representation\n",
    "# This can be used to evaluate different aggregation methods and also provides interpretation of the document representation\n",
    "print([token for (token,_) in model.wv.similar_by_vector(corpus_w2v_avg_clean[0])])\n",
    "\n",
    "# cosine similarity to other documents\n",
    "result=[(1 - cosine(corpus_w2v_avg_clean[0],corpus_w2v_avg_clean[i])) for i in range(1,len(corpus_w2v_avg_clean))]\n",
    "most_similar=data_lemma2[result.index(max(result))+1]\n",
    "print(data_lemma2[0])\n",
    "print('')\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sutGaxDrBite",
    "outputId": "65487fcc-77fb-47c7-ae25-6516ee504284"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11297"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "Rk3iCFMnVPco",
    "outputId": "05233d76-9049-4fb3-9ade-ed5d09031080"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351774</td>\n",
       "      <td>-0.270618</td>\n",
       "      <td>-0.099167</td>\n",
       "      <td>0.440975</td>\n",
       "      <td>0.147224</td>\n",
       "      <td>-0.047850</td>\n",
       "      <td>0.347204</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.306052</td>\n",
       "      <td>-0.533040</td>\n",
       "      <td>0.784328</td>\n",
       "      <td>0.352205</td>\n",
       "      <td>-0.078587</td>\n",
       "      <td>-0.830250</td>\n",
       "      <td>0.189646</td>\n",
       "      <td>-0.017968</td>\n",
       "      <td>0.253096</td>\n",
       "      <td>0.125804</td>\n",
       "      <td>-0.177684</td>\n",
       "      <td>0.034206</td>\n",
       "      <td>0.132728</td>\n",
       "      <td>0.088732</td>\n",
       "      <td>-0.123832</td>\n",
       "      <td>-0.398519</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>-0.270238</td>\n",
       "      <td>0.117907</td>\n",
       "      <td>0.391219</td>\n",
       "      <td>0.158757</td>\n",
       "      <td>0.173804</td>\n",
       "      <td>0.131754</td>\n",
       "      <td>-0.084592</td>\n",
       "      <td>-0.408220</td>\n",
       "      <td>-0.056334</td>\n",
       "      <td>-0.280491</td>\n",
       "      <td>-0.080240</td>\n",
       "      <td>0.259069</td>\n",
       "      <td>0.260064</td>\n",
       "      <td>-0.286499</td>\n",
       "      <td>-0.022011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080258</td>\n",
       "      <td>-0.143452</td>\n",
       "      <td>-0.554667</td>\n",
       "      <td>0.198921</td>\n",
       "      <td>-0.189796</td>\n",
       "      <td>0.216042</td>\n",
       "      <td>-0.360367</td>\n",
       "      <td>0.454792</td>\n",
       "      <td>-0.272879</td>\n",
       "      <td>0.330489</td>\n",
       "      <td>0.399188</td>\n",
       "      <td>0.305223</td>\n",
       "      <td>-0.085042</td>\n",
       "      <td>0.435105</td>\n",
       "      <td>0.282648</td>\n",
       "      <td>0.017710</td>\n",
       "      <td>0.235993</td>\n",
       "      <td>-0.084351</td>\n",
       "      <td>-0.208245</td>\n",
       "      <td>0.192481</td>\n",
       "      <td>-0.132510</td>\n",
       "      <td>0.138845</td>\n",
       "      <td>0.169404</td>\n",
       "      <td>0.136261</td>\n",
       "      <td>-0.032994</td>\n",
       "      <td>-0.172526</td>\n",
       "      <td>-0.098615</td>\n",
       "      <td>0.172062</td>\n",
       "      <td>-0.217515</td>\n",
       "      <td>-0.404443</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>-0.176473</td>\n",
       "      <td>0.651371</td>\n",
       "      <td>0.463280</td>\n",
       "      <td>-0.441628</td>\n",
       "      <td>-0.175508</td>\n",
       "      <td>0.143901</td>\n",
       "      <td>0.502239</td>\n",
       "      <td>-0.043058</td>\n",
       "      <td>-0.491982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.162567</td>\n",
       "      <td>0.056294</td>\n",
       "      <td>0.110691</td>\n",
       "      <td>0.077435</td>\n",
       "      <td>0.173565</td>\n",
       "      <td>0.030174</td>\n",
       "      <td>0.191341</td>\n",
       "      <td>0.136779</td>\n",
       "      <td>-0.149317</td>\n",
       "      <td>0.083286</td>\n",
       "      <td>0.085803</td>\n",
       "      <td>0.400404</td>\n",
       "      <td>-0.347317</td>\n",
       "      <td>0.033176</td>\n",
       "      <td>-0.430936</td>\n",
       "      <td>0.143813</td>\n",
       "      <td>0.261639</td>\n",
       "      <td>-0.115579</td>\n",
       "      <td>-0.062114</td>\n",
       "      <td>-0.088979</td>\n",
       "      <td>-0.347597</td>\n",
       "      <td>-0.098744</td>\n",
       "      <td>-0.244849</td>\n",
       "      <td>-0.298315</td>\n",
       "      <td>0.557203</td>\n",
       "      <td>0.301427</td>\n",
       "      <td>0.090291</td>\n",
       "      <td>-0.113943</td>\n",
       "      <td>0.139821</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>-0.216194</td>\n",
       "      <td>-0.249048</td>\n",
       "      <td>0.365799</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.116574</td>\n",
       "      <td>-0.111931</td>\n",
       "      <td>0.211696</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>-0.092350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006410</td>\n",
       "      <td>0.043633</td>\n",
       "      <td>-0.382989</td>\n",
       "      <td>0.394630</td>\n",
       "      <td>-0.005884</td>\n",
       "      <td>-0.194180</td>\n",
       "      <td>-0.281401</td>\n",
       "      <td>0.188782</td>\n",
       "      <td>-0.145757</td>\n",
       "      <td>0.141406</td>\n",
       "      <td>-0.065778</td>\n",
       "      <td>0.286781</td>\n",
       "      <td>-0.137320</td>\n",
       "      <td>0.399824</td>\n",
       "      <td>0.202061</td>\n",
       "      <td>0.221534</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>-0.190985</td>\n",
       "      <td>0.102994</td>\n",
       "      <td>0.424236</td>\n",
       "      <td>0.098024</td>\n",
       "      <td>0.057525</td>\n",
       "      <td>0.282348</td>\n",
       "      <td>0.437015</td>\n",
       "      <td>0.181157</td>\n",
       "      <td>0.176196</td>\n",
       "      <td>-0.049588</td>\n",
       "      <td>0.107599</td>\n",
       "      <td>0.250988</td>\n",
       "      <td>-0.072748</td>\n",
       "      <td>0.096912</td>\n",
       "      <td>0.354876</td>\n",
       "      <td>0.271327</td>\n",
       "      <td>0.113638</td>\n",
       "      <td>-0.151813</td>\n",
       "      <td>0.036743</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.239501</td>\n",
       "      <td>0.214709</td>\n",
       "      <td>-0.384450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.182770</td>\n",
       "      <td>-0.100424</td>\n",
       "      <td>0.019315</td>\n",
       "      <td>0.334073</td>\n",
       "      <td>0.107887</td>\n",
       "      <td>0.124762</td>\n",
       "      <td>-0.113445</td>\n",
       "      <td>0.359388</td>\n",
       "      <td>-0.018454</td>\n",
       "      <td>-0.086599</td>\n",
       "      <td>0.106137</td>\n",
       "      <td>-0.052467</td>\n",
       "      <td>0.183271</td>\n",
       "      <td>-0.273118</td>\n",
       "      <td>0.272357</td>\n",
       "      <td>0.148054</td>\n",
       "      <td>0.107701</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>-0.053615</td>\n",
       "      <td>-0.050433</td>\n",
       "      <td>-0.063988</td>\n",
       "      <td>-0.272693</td>\n",
       "      <td>-0.252130</td>\n",
       "      <td>-0.030147</td>\n",
       "      <td>0.038941</td>\n",
       "      <td>0.213088</td>\n",
       "      <td>0.046555</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>-0.005292</td>\n",
       "      <td>0.148623</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>-0.080960</td>\n",
       "      <td>-0.206258</td>\n",
       "      <td>0.305082</td>\n",
       "      <td>-0.191143</td>\n",
       "      <td>0.226707</td>\n",
       "      <td>0.109818</td>\n",
       "      <td>-0.082238</td>\n",
       "      <td>0.061433</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198873</td>\n",
       "      <td>0.365456</td>\n",
       "      <td>-0.468160</td>\n",
       "      <td>0.261627</td>\n",
       "      <td>-0.177639</td>\n",
       "      <td>-0.010834</td>\n",
       "      <td>-0.067605</td>\n",
       "      <td>0.086463</td>\n",
       "      <td>-0.112576</td>\n",
       "      <td>0.312297</td>\n",
       "      <td>0.121522</td>\n",
       "      <td>0.208696</td>\n",
       "      <td>-0.096691</td>\n",
       "      <td>0.161278</td>\n",
       "      <td>0.142064</td>\n",
       "      <td>0.319702</td>\n",
       "      <td>0.152462</td>\n",
       "      <td>-0.155592</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.149915</td>\n",
       "      <td>0.032350</td>\n",
       "      <td>-0.038154</td>\n",
       "      <td>-0.102814</td>\n",
       "      <td>0.364744</td>\n",
       "      <td>-0.000543</td>\n",
       "      <td>0.108923</td>\n",
       "      <td>0.129811</td>\n",
       "      <td>0.255257</td>\n",
       "      <td>0.161428</td>\n",
       "      <td>0.131891</td>\n",
       "      <td>0.146105</td>\n",
       "      <td>-0.147394</td>\n",
       "      <td>0.027552</td>\n",
       "      <td>-0.035525</td>\n",
       "      <td>-0.278403</td>\n",
       "      <td>0.124541</td>\n",
       "      <td>0.122495</td>\n",
       "      <td>0.117965</td>\n",
       "      <td>0.404937</td>\n",
       "      <td>-0.070498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.294891</td>\n",
       "      <td>0.211890</td>\n",
       "      <td>0.061987</td>\n",
       "      <td>-0.018384</td>\n",
       "      <td>-0.002906</td>\n",
       "      <td>-0.085502</td>\n",
       "      <td>-0.344018</td>\n",
       "      <td>0.004979</td>\n",
       "      <td>0.080335</td>\n",
       "      <td>-0.303516</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>-0.112059</td>\n",
       "      <td>0.218604</td>\n",
       "      <td>-0.104201</td>\n",
       "      <td>0.311974</td>\n",
       "      <td>0.140519</td>\n",
       "      <td>0.176662</td>\n",
       "      <td>0.226410</td>\n",
       "      <td>-0.109118</td>\n",
       "      <td>0.018592</td>\n",
       "      <td>0.088117</td>\n",
       "      <td>-0.046050</td>\n",
       "      <td>0.132062</td>\n",
       "      <td>-0.438827</td>\n",
       "      <td>-0.211266</td>\n",
       "      <td>0.215469</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>-0.195149</td>\n",
       "      <td>-0.235789</td>\n",
       "      <td>-0.169567</td>\n",
       "      <td>0.054132</td>\n",
       "      <td>-0.428622</td>\n",
       "      <td>-0.032860</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>0.092482</td>\n",
       "      <td>0.606193</td>\n",
       "      <td>0.226337</td>\n",
       "      <td>-0.272084</td>\n",
       "      <td>-0.146256</td>\n",
       "      <td>0.248828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134156</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>-0.340334</td>\n",
       "      <td>0.188290</td>\n",
       "      <td>0.200545</td>\n",
       "      <td>-0.219851</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>-0.140909</td>\n",
       "      <td>0.125717</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>-0.062580</td>\n",
       "      <td>-0.022784</td>\n",
       "      <td>0.048569</td>\n",
       "      <td>0.217845</td>\n",
       "      <td>0.114779</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>-0.327130</td>\n",
       "      <td>-0.017834</td>\n",
       "      <td>0.166170</td>\n",
       "      <td>-0.104674</td>\n",
       "      <td>-0.128521</td>\n",
       "      <td>0.199192</td>\n",
       "      <td>0.021575</td>\n",
       "      <td>0.033676</td>\n",
       "      <td>-0.036859</td>\n",
       "      <td>0.058534</td>\n",
       "      <td>0.236719</td>\n",
       "      <td>0.294604</td>\n",
       "      <td>0.166780</td>\n",
       "      <td>-0.229979</td>\n",
       "      <td>-0.164436</td>\n",
       "      <td>0.149876</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>-0.437927</td>\n",
       "      <td>0.156925</td>\n",
       "      <td>-0.166397</td>\n",
       "      <td>0.117495</td>\n",
       "      <td>0.078160</td>\n",
       "      <td>-0.370750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.217928</td>\n",
       "      <td>-0.257206</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>-0.010809</td>\n",
       "      <td>0.019237</td>\n",
       "      <td>0.066630</td>\n",
       "      <td>-0.420439</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.172918</td>\n",
       "      <td>-0.076680</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>-0.287048</td>\n",
       "      <td>0.222951</td>\n",
       "      <td>-0.186517</td>\n",
       "      <td>0.071781</td>\n",
       "      <td>0.229308</td>\n",
       "      <td>-0.063299</td>\n",
       "      <td>-0.108122</td>\n",
       "      <td>0.037440</td>\n",
       "      <td>0.042467</td>\n",
       "      <td>-0.086294</td>\n",
       "      <td>0.102709</td>\n",
       "      <td>-0.073687</td>\n",
       "      <td>0.047708</td>\n",
       "      <td>-0.205369</td>\n",
       "      <td>0.355595</td>\n",
       "      <td>0.095391</td>\n",
       "      <td>-0.552123</td>\n",
       "      <td>-0.062507</td>\n",
       "      <td>-0.062732</td>\n",
       "      <td>-0.336270</td>\n",
       "      <td>-0.186323</td>\n",
       "      <td>-0.155956</td>\n",
       "      <td>0.449449</td>\n",
       "      <td>-0.010485</td>\n",
       "      <td>0.703118</td>\n",
       "      <td>0.079991</td>\n",
       "      <td>-0.158513</td>\n",
       "      <td>-0.078529</td>\n",
       "      <td>-0.176440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187346</td>\n",
       "      <td>0.243681</td>\n",
       "      <td>-0.329644</td>\n",
       "      <td>0.080310</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>-0.151513</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>-0.358037</td>\n",
       "      <td>-0.188895</td>\n",
       "      <td>0.193158</td>\n",
       "      <td>-0.279578</td>\n",
       "      <td>-0.007327</td>\n",
       "      <td>0.048084</td>\n",
       "      <td>0.107231</td>\n",
       "      <td>0.098582</td>\n",
       "      <td>0.161298</td>\n",
       "      <td>-0.211673</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>0.013817</td>\n",
       "      <td>0.146621</td>\n",
       "      <td>-0.012112</td>\n",
       "      <td>-0.090765</td>\n",
       "      <td>-0.194526</td>\n",
       "      <td>0.116612</td>\n",
       "      <td>0.153502</td>\n",
       "      <td>0.167925</td>\n",
       "      <td>0.123219</td>\n",
       "      <td>0.106242</td>\n",
       "      <td>0.257059</td>\n",
       "      <td>0.200487</td>\n",
       "      <td>-0.486795</td>\n",
       "      <td>0.091707</td>\n",
       "      <td>0.082330</td>\n",
       "      <td>0.116555</td>\n",
       "      <td>-0.291279</td>\n",
       "      <td>0.382398</td>\n",
       "      <td>-0.324631</td>\n",
       "      <td>-0.012788</td>\n",
       "      <td>0.022908</td>\n",
       "      <td>-0.307021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2   ...        97        98        99\n",
       "0  0.351774 -0.270618 -0.099167  ...  0.502239 -0.043058 -0.491982\n",
       "1  0.008163  0.162567  0.056294  ...  0.239501  0.214709 -0.384450\n",
       "2  0.182770 -0.100424  0.019315  ...  0.117965  0.404937 -0.070498\n",
       "3 -0.294891  0.211890  0.061987  ...  0.117495  0.078160 -0.370750\n",
       "4 -0.217928 -0.257206  0.003999  ... -0.012788  0.022908 -0.307021\n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corpus as data frame that can be used in downstream tasks such as classification\n",
    "corpus_w2v_avg_df=pd.DataFrame(corpus_w2v_avg_clean)\n",
    "corpus_w2v_avg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1EFSaifCauV",
    "outputId": "b857d13c-5683-4b36-a425-68f09d1b4331"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11298"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_w2v_avg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_xU9j17DYQu-"
   },
   "outputs": [],
   "source": [
    "pickle.dump(corpus_w2v_avg_df, open(\"/content/drive/MyDrive/TWSM_Data/WordtoVecModel.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1FM2CMTQf8w"
   },
   "source": [
    "## 2. Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Ar_f51MHQZL3"
   },
   "outputs": [],
   "source": [
    "# Run doc2vec on the tagged texts\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_gen)]\n",
    "model2 = Doc2Vec(documents, vector_size=100, min_count=566)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUm1Bfw_R_h_",
    "outputId": "3f5ba290-420b-4e1a-deec-83c7181d1a78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05122636, -0.06806806, -0.01494412,  0.01662492, -0.0822029 ,\n",
       "       -0.02819622, -0.02895089, -0.06420164, -0.04436788, -0.16889516,\n",
       "        0.06832569,  0.00772923,  0.0206603 , -0.10237534,  0.06752286,\n",
       "       -0.03361692,  0.00732967, -0.01266144, -0.03883754, -0.01195568,\n",
       "        0.01666871, -0.06581076, -0.03252764, -0.03990467,  0.02596428,\n",
       "       -0.05007704, -0.02736584,  0.06371695,  0.01925586, -0.04054766,\n",
       "        0.02015869, -0.08511249, -0.01322431, -0.00986988,  0.02593201,\n",
       "        0.05694926,  0.02162211, -0.00424531,  0.02517235,  0.04707081,\n",
       "       -0.03879257,  0.07847797, -0.06109332, -0.06253222, -0.0751923 ,\n",
       "       -0.0168055 ,  0.05226422, -0.06491261, -0.03435625,  0.0289027 ,\n",
       "        0.01662089, -0.00954358, -0.05242245,  0.05766357,  0.09904396,\n",
       "        0.04716758,  0.00344208,  0.02078508, -0.02033357, -0.00767193,\n",
       "       -0.01240751, -0.0629037 , -0.06261925,  0.04491214,  0.03900221,\n",
       "        0.01412876, -0.034928  ,  0.04426256, -0.0885656 ,  0.05429733,\n",
       "        0.00397082,  0.02777545, -0.00596366,  0.11242855,  0.04913192,\n",
       "       -0.04914148,  0.00560875, -0.02579891, -0.02182602,  0.12941292,\n",
       "       -0.00198071,  0.00650514,  0.02494005, -0.00346044,  0.02152675,\n",
       "        0.04677475, -0.05368706,  0.01968129, -0.05359885, -0.01763162,\n",
       "       -0.03452906, -0.01264213,  0.08926452,  0.01396446, -0.0683023 ,\n",
       "       -0.02519521, -0.03034328,  0.07302649, -0.02946503, -0.12121043],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding for the first document\n",
    "vector = model2.infer_vector(corpus_gen[0])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4YP1drRDCLe",
    "outputId": "e96d45d1-ffdf-4f6b-dc5a-5d640cb71c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine specs year production car history info funky looking car mail thank\n",
      "\n",
      "trading car pay pointer article fibercom com rrg rtp fibercom com rhonda gaines write plan purchase new car trading mazda get year pay take account purchase new car dealership pay car add pay purchase price new car explain know bank credit union finance company hold loan present car current payoff cost trading current car new car subtract payoff trade dealer give turn negative number need reconsider deal subtract difference price new car size loan need new car dealer care pay loan old car money pick new car work year ago ohio thank rhonda joseph staudt telxon corp joes telxon com box usenet like tetris people akron remember read heller\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cosine similarity to other documents\n",
    "result=[(1 - cosine(vector,model2.infer_vector(corpus_gen[i]))) for i in range(1,len(corpus_gen))]\n",
    "most_similar=data_lemma[result.index(max(result))+1]\n",
    "\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVKFlkydDy4P",
    "outputId": "6502c461-999c-4c1d-a968-ffa3c2d1578f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "tmyNi6XVU2o-"
   },
   "outputs": [],
   "source": [
    "# Final corpus for classification\n",
    "corpus_d2v=pd.DataFrame([model2.infer_vector(doc) for doc in corpus_gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "P2SVegMSVP0e",
    "outputId": "8a522135-5293-42d0-cb8d-3495e9a55803"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.033262</td>\n",
       "      <td>-0.049674</td>\n",
       "      <td>-0.016017</td>\n",
       "      <td>-0.001603</td>\n",
       "      <td>-0.072937</td>\n",
       "      <td>-0.005914</td>\n",
       "      <td>-0.026601</td>\n",
       "      <td>-0.045852</td>\n",
       "      <td>-0.016930</td>\n",
       "      <td>-0.113542</td>\n",
       "      <td>0.031533</td>\n",
       "      <td>0.018571</td>\n",
       "      <td>0.016846</td>\n",
       "      <td>-0.076718</td>\n",
       "      <td>0.076404</td>\n",
       "      <td>-0.023871</td>\n",
       "      <td>-0.025500</td>\n",
       "      <td>-0.009350</td>\n",
       "      <td>-0.024001</td>\n",
       "      <td>-0.007181</td>\n",
       "      <td>0.020286</td>\n",
       "      <td>-0.081678</td>\n",
       "      <td>-0.031725</td>\n",
       "      <td>-0.052242</td>\n",
       "      <td>0.019910</td>\n",
       "      <td>-0.020617</td>\n",
       "      <td>-0.027672</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>-0.037598</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>-0.072705</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>-0.003011</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>0.049824</td>\n",
       "      <td>0.028008</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.019915</td>\n",
       "      <td>0.065455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025305</td>\n",
       "      <td>-0.023110</td>\n",
       "      <td>-0.058257</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>-0.013546</td>\n",
       "      <td>-0.008874</td>\n",
       "      <td>0.044304</td>\n",
       "      <td>-0.064030</td>\n",
       "      <td>0.055633</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>0.013020</td>\n",
       "      <td>-0.019584</td>\n",
       "      <td>0.092485</td>\n",
       "      <td>0.051242</td>\n",
       "      <td>-0.019956</td>\n",
       "      <td>-0.005542</td>\n",
       "      <td>-0.038789</td>\n",
       "      <td>0.012394</td>\n",
       "      <td>0.112057</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>-0.018436</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.028916</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>-0.054600</td>\n",
       "      <td>0.023579</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>-0.010699</td>\n",
       "      <td>-0.040269</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.076182</td>\n",
       "      <td>0.014719</td>\n",
       "      <td>-0.048158</td>\n",
       "      <td>-0.020135</td>\n",
       "      <td>-0.013903</td>\n",
       "      <td>0.057317</td>\n",
       "      <td>-0.019900</td>\n",
       "      <td>-0.095958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033676</td>\n",
       "      <td>-0.049887</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>0.026024</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.031365</td>\n",
       "      <td>0.043312</td>\n",
       "      <td>0.064511</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.025403</td>\n",
       "      <td>-0.002454</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>-0.006850</td>\n",
       "      <td>-0.014350</td>\n",
       "      <td>-0.055728</td>\n",
       "      <td>0.030716</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>-0.016803</td>\n",
       "      <td>-0.014938</td>\n",
       "      <td>-0.038310</td>\n",
       "      <td>-0.002253</td>\n",
       "      <td>-0.022165</td>\n",
       "      <td>-0.052017</td>\n",
       "      <td>0.081576</td>\n",
       "      <td>0.035850</td>\n",
       "      <td>-0.030607</td>\n",
       "      <td>-0.053455</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>-0.038902</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>-0.053146</td>\n",
       "      <td>0.049171</td>\n",
       "      <td>0.023251</td>\n",
       "      <td>-0.009203</td>\n",
       "      <td>-0.067071</td>\n",
       "      <td>0.063215</td>\n",
       "      <td>-0.025070</td>\n",
       "      <td>-0.054228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021282</td>\n",
       "      <td>-0.060994</td>\n",
       "      <td>-0.010744</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>-0.016958</td>\n",
       "      <td>-0.017521</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>-0.019094</td>\n",
       "      <td>-0.016233</td>\n",
       "      <td>-0.008472</td>\n",
       "      <td>0.036808</td>\n",
       "      <td>0.034624</td>\n",
       "      <td>-0.040458</td>\n",
       "      <td>0.019580</td>\n",
       "      <td>0.060933</td>\n",
       "      <td>0.037337</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.081936</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.027454</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.051145</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.026966</td>\n",
       "      <td>-0.018686</td>\n",
       "      <td>-0.037749</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.014192</td>\n",
       "      <td>-0.030557</td>\n",
       "      <td>0.052317</td>\n",
       "      <td>0.027663</td>\n",
       "      <td>-0.025419</td>\n",
       "      <td>-0.022255</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>-0.031640</td>\n",
       "      <td>0.009603</td>\n",
       "      <td>0.051909</td>\n",
       "      <td>-0.120971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042794</td>\n",
       "      <td>-0.064218</td>\n",
       "      <td>0.037469</td>\n",
       "      <td>0.093485</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.061478</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>0.193311</td>\n",
       "      <td>-0.005121</td>\n",
       "      <td>0.018810</td>\n",
       "      <td>-0.039404</td>\n",
       "      <td>-0.109424</td>\n",
       "      <td>-0.021143</td>\n",
       "      <td>-0.039642</td>\n",
       "      <td>0.025812</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.051470</td>\n",
       "      <td>-0.079580</td>\n",
       "      <td>0.020494</td>\n",
       "      <td>0.047878</td>\n",
       "      <td>-0.039136</td>\n",
       "      <td>-0.059271</td>\n",
       "      <td>-0.165079</td>\n",
       "      <td>0.074292</td>\n",
       "      <td>-0.091949</td>\n",
       "      <td>0.158054</td>\n",
       "      <td>-0.017687</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.075507</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>-0.064246</td>\n",
       "      <td>-0.012298</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.076071</td>\n",
       "      <td>0.138779</td>\n",
       "      <td>0.048218</td>\n",
       "      <td>-0.011923</td>\n",
       "      <td>-0.065729</td>\n",
       "      <td>-0.025097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>0.144704</td>\n",
       "      <td>-0.073482</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.015396</td>\n",
       "      <td>0.022823</td>\n",
       "      <td>-0.119585</td>\n",
       "      <td>-0.052392</td>\n",
       "      <td>0.034922</td>\n",
       "      <td>0.130359</td>\n",
       "      <td>0.145847</td>\n",
       "      <td>-0.048500</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>-0.040287</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>0.059772</td>\n",
       "      <td>-0.017194</td>\n",
       "      <td>-0.084103</td>\n",
       "      <td>0.049472</td>\n",
       "      <td>0.164592</td>\n",
       "      <td>0.057187</td>\n",
       "      <td>0.071866</td>\n",
       "      <td>-0.005872</td>\n",
       "      <td>0.117578</td>\n",
       "      <td>0.090215</td>\n",
       "      <td>0.102958</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>-0.034454</td>\n",
       "      <td>-0.038481</td>\n",
       "      <td>0.036949</td>\n",
       "      <td>0.084788</td>\n",
       "      <td>-0.066470</td>\n",
       "      <td>-0.023559</td>\n",
       "      <td>0.055390</td>\n",
       "      <td>-0.037519</td>\n",
       "      <td>0.115474</td>\n",
       "      <td>0.023027</td>\n",
       "      <td>0.075927</td>\n",
       "      <td>0.156555</td>\n",
       "      <td>-0.033438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058012</td>\n",
       "      <td>0.085990</td>\n",
       "      <td>0.036921</td>\n",
       "      <td>0.074976</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>-0.034835</td>\n",
       "      <td>-0.044319</td>\n",
       "      <td>-0.029044</td>\n",
       "      <td>0.054942</td>\n",
       "      <td>-0.036614</td>\n",
       "      <td>0.044074</td>\n",
       "      <td>-0.005479</td>\n",
       "      <td>0.033930</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>-0.007639</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>0.040478</td>\n",
       "      <td>0.107811</td>\n",
       "      <td>0.012438</td>\n",
       "      <td>0.020723</td>\n",
       "      <td>0.018761</td>\n",
       "      <td>-0.085365</td>\n",
       "      <td>0.060698</td>\n",
       "      <td>-0.024213</td>\n",
       "      <td>-0.011794</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.023021</td>\n",
       "      <td>0.043049</td>\n",
       "      <td>-0.029760</td>\n",
       "      <td>-0.006028</td>\n",
       "      <td>0.019067</td>\n",
       "      <td>-0.041945</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.068180</td>\n",
       "      <td>0.027937</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.086288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026592</td>\n",
       "      <td>0.030936</td>\n",
       "      <td>-0.044287</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.034602</td>\n",
       "      <td>-0.039755</td>\n",
       "      <td>-0.019350</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.038782</td>\n",
       "      <td>0.007920</td>\n",
       "      <td>-0.006176</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>-0.052613</td>\n",
       "      <td>0.022979</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>-0.009055</td>\n",
       "      <td>0.059402</td>\n",
       "      <td>-0.070373</td>\n",
       "      <td>-0.008902</td>\n",
       "      <td>0.040143</td>\n",
       "      <td>-0.011268</td>\n",
       "      <td>-0.019510</td>\n",
       "      <td>-0.044945</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>-0.023231</td>\n",
       "      <td>0.075448</td>\n",
       "      <td>-0.042095</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.098185</td>\n",
       "      <td>0.032535</td>\n",
       "      <td>-0.033829</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>-0.026878</td>\n",
       "      <td>0.022225</td>\n",
       "      <td>-0.126712</td>\n",
       "      <td>0.017064</td>\n",
       "      <td>-0.001792</td>\n",
       "      <td>0.023652</td>\n",
       "      <td>0.031867</td>\n",
       "      <td>-0.029106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010468</td>\n",
       "      <td>-0.031585</td>\n",
       "      <td>0.066272</td>\n",
       "      <td>0.047936</td>\n",
       "      <td>-0.008234</td>\n",
       "      <td>-0.043216</td>\n",
       "      <td>-0.079181</td>\n",
       "      <td>0.037457</td>\n",
       "      <td>-0.020777</td>\n",
       "      <td>-0.045592</td>\n",
       "      <td>0.013054</td>\n",
       "      <td>-0.006131</td>\n",
       "      <td>0.090572</td>\n",
       "      <td>-0.037595</td>\n",
       "      <td>0.025614</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>-0.075415</td>\n",
       "      <td>-0.015045</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>0.072738</td>\n",
       "      <td>-0.026502</td>\n",
       "      <td>-0.113079</td>\n",
       "      <td>0.017322</td>\n",
       "      <td>-0.013613</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.104365</td>\n",
       "      <td>0.014429</td>\n",
       "      <td>-0.032148</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>-0.008472</td>\n",
       "      <td>-0.011973</td>\n",
       "      <td>-0.009424</td>\n",
       "      <td>0.084865</td>\n",
       "      <td>-0.003347</td>\n",
       "      <td>0.104144</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>0.031158</td>\n",
       "      <td>-0.037503</td>\n",
       "      <td>0.027462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048934</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>-0.049480</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>-0.092805</td>\n",
       "      <td>0.009015</td>\n",
       "      <td>0.100562</td>\n",
       "      <td>-0.073604</td>\n",
       "      <td>-0.044452</td>\n",
       "      <td>0.039012</td>\n",
       "      <td>-0.015884</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>0.038119</td>\n",
       "      <td>0.017468</td>\n",
       "      <td>0.084680</td>\n",
       "      <td>-0.031270</td>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.146343</td>\n",
       "      <td>-0.038083</td>\n",
       "      <td>-0.079363</td>\n",
       "      <td>-0.114853</td>\n",
       "      <td>0.083276</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.085579</td>\n",
       "      <td>-0.015446</td>\n",
       "      <td>0.033404</td>\n",
       "      <td>0.061184</td>\n",
       "      <td>0.052265</td>\n",
       "      <td>-0.072259</td>\n",
       "      <td>0.043620</td>\n",
       "      <td>-0.017970</td>\n",
       "      <td>0.092339</td>\n",
       "      <td>-0.110279</td>\n",
       "      <td>0.027145</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>0.047018</td>\n",
       "      <td>-0.018687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2   ...        97        98        99\n",
       "0 -0.033262 -0.049674 -0.016017  ...  0.057317 -0.019900 -0.095958\n",
       "1  0.033676 -0.049887  0.027683  ...  0.009603  0.051909 -0.120971\n",
       "2  0.042794 -0.064218  0.037469  ...  0.075927  0.156555 -0.033438\n",
       "3  0.058012  0.085990  0.036921  ...  0.023652  0.031867 -0.029106\n",
       "4  0.010468 -0.031585  0.066272  ...  0.029041  0.047018 -0.018687\n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_d2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "z4hnYK7pZdwn"
   },
   "outputs": [],
   "source": [
    "pickle.dump(corpus_d2v, open(\"/content/drive/MyDrive/TWSM_Data/DoctoVecModel.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma9iL0Y-vJAG"
   },
   "source": [
    "## 3. BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PutarxWsbKPc"
   },
   "source": [
    "Confirm that GPU is detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8Rl80AGbIcQ",
    "outputId": "db51b5d2-2f89-46f4-865d-eb05af1943da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg1j-HN2bhIl"
   },
   "source": [
    "Assign the GPU device to torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UrucJQXVbgie",
    "outputId": "f7a81a7e-d3c1-485a-82a7-7d6ab87f498b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvgLCEFzvUYU"
   },
   "source": [
    "In order to apply BERT, we need to derive three data objects for the text data:\n",
    "1. Add [CLS] at the beginning and [SEP] at the end of each text. [SEP] is a legacy from teh model training. The result for [CLS] is then used later as document representation for classification tasks.\n",
    "2. Tokenize the texts using BERT tokenizer\n",
    "3. Pad or truncate the text to the maximum length (maximum 512)\n",
    "4. Map the remaining tokens to BERT dictionary \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfsZbJTovPAk",
    "outputId": "0fdb229a-a620-4934-e931-bebe19a1d4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] car wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine specs year production car history info funky looking car mail thank [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 1. Add [CLS] at the beginning and [SEP] at the end of each text.\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in data_lemma]\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVhJhuTNw3vm",
    "outputId": "91153ca0-7b05-498b-d19d-c1c9e70660e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'car', 'wonder', 'en', '##light', '##en', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'brick', '##lin', 'door', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tell', '##me', 'model', 'engine', 'spec', '##s', 'year', 'production', 'car', 'history', 'info', 'funky', 'looking', 'car', 'mail', 'thank', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokenize the texts using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cC_lwMCmdurn",
    "outputId": "37567107-b78e-4c7f-b65a-73d0871f934a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2482, 4687, 4372, 7138, 2368, 2482, 2156, 2154, 2341, 4368, 2482, 2298, 2397, 2220, 2655, 5318, 4115, 2341, 2235, 2804, 21519, 3584, 2717, 2303, 2113, 2425, 4168, 2944, 3194, 28699, 2015, 2095, 2537, 2482, 2381, 18558, 24151, 2559, 2482, 5653, 4067, 102]\n"
     ]
    }
   ],
   "source": [
    "# Show token IDs based on BERT's training\n",
    "print(tokenizer.convert_tokens_to_ids(tokenized_texts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS5uylwxySbP"
   },
   "source": [
    "In order to determine the maximum sequence length, we look at the list statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "6pEoPoTIkmge",
    "outputId": "1b19d903-77c1-42b1-c015-506ff084b54e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>170.796182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>394.871090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8235.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  11314.000000\n",
       "mean     170.796182\n",
       "std      394.871090\n",
       "min        4.000000\n",
       "25%       63.000000\n",
       "50%      103.000000\n",
       "75%      166.000000\n",
       "max     8235.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng=[]\n",
    "for t in tokenized_texts:\n",
    "  leng.append(len(t))\n",
    "df=pd.DataFrame(leng)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "Y-bq1lCQn4vK",
    "outputId": "e0b6a4ea-f18d-417d-88ee-461caf137031"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>412.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>1304.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0.95   412.00\n",
       "0.99  1304.09"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quantile([.95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMXYZrCRxCu2",
    "outputId": "f82925ea-1026-42de-a6d7-e9ab0bfa51a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]' 'car' 'wonder' 'en' '##light' '##en' 'car' 'see' 'day' 'door'\n",
      " 'sport' 'car' 'look' 'late' 'early' 'call' 'brick' '##lin' 'door' 'small'\n",
      " 'addition' 'bumper' 'separate' 'rest' 'body' 'know' 'tell' '##me' 'model'\n",
      " 'engine' 'spec' '##s' 'year' 'production' 'car' 'history' 'info' 'funky'\n",
      " 'looking' 'car' 'mail' 'thank' '[SEP]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]'\n",
      " '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 3. Pad the text to the maximum length, max 512\n",
    "\n",
    "# Pad sequences that are less than MAX_LEN, if more, remove from the end\n",
    "sentences_padded = pad_sequences(tokenized_texts,  dtype=object,maxlen=412,  value='[PAD]', truncating=\"post\",padding=\"post\", return_tensors = 'pt')\n",
    "print(sentences_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSorqlkhv792",
    "outputId": "d56e3780-5fc8-4916-d01f-b829bf940209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2482, 4687, 4372, 7138, 2368, 2482, 2156, 2154, 2341, 4368, 2482, 2298, 2397, 2220, 2655, 5318, 4115, 2341, 2235, 2804, 21519, 3584, 2717, 2303, 2113, 2425, 4168, 2944, 3194, 28699, 2015, 2095, 2537, 2482, 2381, 18558, 24151, 2559, 2482, 5653, 4067, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#4. Map the tokens to BERT dictionary \n",
    "# Convert the tokens to their index numbers in the BERT vocabulary\n",
    "sentences_converted = [tokenizer.convert_tokens_to_ids(s) for s in sentences_padded]\n",
    "print(sentences_converted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyiBA71874Ww"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in sentences_converted:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga2etroyhmCg"
   },
   "outputs": [],
   "source": [
    "# 5. Generate embeddings\n",
    "\n",
    "#Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "inputs = torch.LongTensor(sentences_converted)\n",
    "masks = torch.LongTensor(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU3Bc9vcrf3l",
    "outputId": "bee37f0c-c3a0-402f-d11c-aa8e5ea391a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11314, 412])"
      ]
     },
     "execution_count": 171,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scHjWdSdAgVI",
    "outputId": "5c8cab4a-350d-4c79-c5b0-075aa69ca584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11314, 412])"
      ]
     },
     "execution_count": 176,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Unk7mmko2Y2_"
   },
   "outputs": [],
   "source": [
    "# Apply Pretrained model to the sentences\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REYqHgGy60sW",
    "outputId": "36265cc7-6015-4cc3-a476-3ce2636d8b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5jVGRi6tMJq"
   },
   "outputs": [],
   "source": [
    "# Set the batch size.  \n",
    "batch_size = 16  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(inputs, masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RH09MuNavJ9u"
   },
   "outputs": [],
   "source": [
    "result=[]\n",
    "i=0\n",
    "for batch in prediction_dataloader:\n",
    "  #print(i)\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask = batch\n",
    "\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  \n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate embeddings\n",
    "      outputs = model(b_input_ids)\n",
    "\n",
    "  embeddings = outputs.pooler_output #CLS embeddings for the batch\n",
    "\n",
    "  # Move em to CPU\n",
    "  embeddings = embeddings.detach().cpu().numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  result.append(embeddings)\n",
    "  i=i+1\n",
    "\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2V2cxivqCTDf",
    "outputId": "2ab1e5bc-f667-45a5-8e64-86852d46389c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 186,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#708 batches*16 texts with embedding size 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFtgYQZRCxXK"
   },
   "outputs": [],
   "source": [
    "final=[]\n",
    "for b in result:\n",
    "   for e in b:\n",
    "      final.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "ZnhbzdZYMFAA",
    "outputId": "2e8f8232-f324-4c53-ebe8-5e4b52c9a3ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.120891</td>\n",
       "      <td>-0.280994</td>\n",
       "      <td>-0.963245</td>\n",
       "      <td>0.419622</td>\n",
       "      <td>0.778366</td>\n",
       "      <td>-0.196418</td>\n",
       "      <td>-0.313293</td>\n",
       "      <td>0.232870</td>\n",
       "      <td>-0.862499</td>\n",
       "      <td>-0.955871</td>\n",
       "      <td>0.223536</td>\n",
       "      <td>0.886177</td>\n",
       "      <td>0.327486</td>\n",
       "      <td>0.866635</td>\n",
       "      <td>-0.249378</td>\n",
       "      <td>0.088374</td>\n",
       "      <td>0.439759</td>\n",
       "      <td>0.016313</td>\n",
       "      <td>0.064368</td>\n",
       "      <td>0.730549</td>\n",
       "      <td>0.462973</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>-0.408310</td>\n",
       "      <td>0.307045</td>\n",
       "      <td>0.337647</td>\n",
       "      <td>0.908904</td>\n",
       "      <td>-0.068019</td>\n",
       "      <td>-0.055471</td>\n",
       "      <td>0.252830</td>\n",
       "      <td>0.328617</td>\n",
       "      <td>0.393255</td>\n",
       "      <td>0.119468</td>\n",
       "      <td>-0.622634</td>\n",
       "      <td>-0.270945</td>\n",
       "      <td>-0.978561</td>\n",
       "      <td>-0.187582</td>\n",
       "      <td>0.180457</td>\n",
       "      <td>0.127950</td>\n",
       "      <td>-0.101807</td>\n",
       "      <td>-0.250290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428010</td>\n",
       "      <td>-0.247050</td>\n",
       "      <td>-0.033118</td>\n",
       "      <td>-0.254504</td>\n",
       "      <td>-0.322066</td>\n",
       "      <td>-0.023203</td>\n",
       "      <td>-0.252481</td>\n",
       "      <td>-0.285563</td>\n",
       "      <td>0.193448</td>\n",
       "      <td>0.033795</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>-0.736765</td>\n",
       "      <td>-0.848266</td>\n",
       "      <td>-0.186036</td>\n",
       "      <td>-0.336793</td>\n",
       "      <td>0.301686</td>\n",
       "      <td>-0.462011</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.255416</td>\n",
       "      <td>-0.859095</td>\n",
       "      <td>0.803370</td>\n",
       "      <td>-0.318131</td>\n",
       "      <td>0.902804</td>\n",
       "      <td>-0.754084</td>\n",
       "      <td>0.307099</td>\n",
       "      <td>-0.061251</td>\n",
       "      <td>0.624030</td>\n",
       "      <td>0.844899</td>\n",
       "      <td>-0.087035</td>\n",
       "      <td>-0.475938</td>\n",
       "      <td>0.161414</td>\n",
       "      <td>-0.928535</td>\n",
       "      <td>0.863065</td>\n",
       "      <td>-0.074633</td>\n",
       "      <td>-0.002260</td>\n",
       "      <td>-0.675741</td>\n",
       "      <td>0.185203</td>\n",
       "      <td>-0.855487</td>\n",
       "      <td>-0.178540</td>\n",
       "      <td>-0.240451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.183047</td>\n",
       "      <td>-0.320446</td>\n",
       "      <td>-0.967441</td>\n",
       "      <td>0.510457</td>\n",
       "      <td>0.806681</td>\n",
       "      <td>-0.212261</td>\n",
       "      <td>-0.239628</td>\n",
       "      <td>0.210489</td>\n",
       "      <td>-0.880438</td>\n",
       "      <td>-0.945462</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>0.914262</td>\n",
       "      <td>0.214913</td>\n",
       "      <td>0.882294</td>\n",
       "      <td>-0.172117</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.382229</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.700129</td>\n",
       "      <td>0.484552</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>-0.460843</td>\n",
       "      <td>0.301718</td>\n",
       "      <td>0.323201</td>\n",
       "      <td>0.921945</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.070382</td>\n",
       "      <td>0.238025</td>\n",
       "      <td>0.354671</td>\n",
       "      <td>0.378942</td>\n",
       "      <td>0.101619</td>\n",
       "      <td>-0.541697</td>\n",
       "      <td>-0.292166</td>\n",
       "      <td>-0.978894</td>\n",
       "      <td>-0.117593</td>\n",
       "      <td>0.185518</td>\n",
       "      <td>0.117982</td>\n",
       "      <td>-0.145663</td>\n",
       "      <td>-0.236753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481052</td>\n",
       "      <td>-0.270803</td>\n",
       "      <td>-0.048760</td>\n",
       "      <td>-0.217497</td>\n",
       "      <td>-0.346388</td>\n",
       "      <td>0.035731</td>\n",
       "      <td>-0.253299</td>\n",
       "      <td>-0.326463</td>\n",
       "      <td>0.175167</td>\n",
       "      <td>-0.009337</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>-0.775669</td>\n",
       "      <td>-0.875688</td>\n",
       "      <td>-0.178330</td>\n",
       "      <td>-0.360491</td>\n",
       "      <td>0.283788</td>\n",
       "      <td>-0.451560</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.240596</td>\n",
       "      <td>-0.880769</td>\n",
       "      <td>0.822632</td>\n",
       "      <td>-0.391792</td>\n",
       "      <td>0.911289</td>\n",
       "      <td>-0.800638</td>\n",
       "      <td>0.229179</td>\n",
       "      <td>-0.054229</td>\n",
       "      <td>0.605605</td>\n",
       "      <td>0.866382</td>\n",
       "      <td>-0.109120</td>\n",
       "      <td>-0.466791</td>\n",
       "      <td>0.175150</td>\n",
       "      <td>-0.939977</td>\n",
       "      <td>0.888713</td>\n",
       "      <td>-0.089563</td>\n",
       "      <td>-0.091914</td>\n",
       "      <td>-0.731656</td>\n",
       "      <td>0.169998</td>\n",
       "      <td>-0.887561</td>\n",
       "      <td>-0.124455</td>\n",
       "      <td>-0.239807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.447093</td>\n",
       "      <td>-0.470325</td>\n",
       "      <td>-0.965326</td>\n",
       "      <td>0.468330</td>\n",
       "      <td>0.782669</td>\n",
       "      <td>-0.307996</td>\n",
       "      <td>-0.073427</td>\n",
       "      <td>0.399358</td>\n",
       "      <td>-0.898938</td>\n",
       "      <td>-0.996745</td>\n",
       "      <td>-0.013201</td>\n",
       "      <td>0.910904</td>\n",
       "      <td>0.592074</td>\n",
       "      <td>0.867216</td>\n",
       "      <td>0.129251</td>\n",
       "      <td>-0.309642</td>\n",
       "      <td>0.301718</td>\n",
       "      <td>-0.296469</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>0.562351</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>-0.372397</td>\n",
       "      <td>0.492953</td>\n",
       "      <td>0.458219</td>\n",
       "      <td>0.939361</td>\n",
       "      <td>-0.354881</td>\n",
       "      <td>0.247452</td>\n",
       "      <td>0.618205</td>\n",
       "      <td>0.554835</td>\n",
       "      <td>0.122690</td>\n",
       "      <td>0.320390</td>\n",
       "      <td>-0.797047</td>\n",
       "      <td>-0.401551</td>\n",
       "      <td>-0.978959</td>\n",
       "      <td>-0.659457</td>\n",
       "      <td>0.303599</td>\n",
       "      <td>-0.156353</td>\n",
       "      <td>-0.214980</td>\n",
       "      <td>-0.144441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>-0.322595</td>\n",
       "      <td>-0.211455</td>\n",
       "      <td>-0.266762</td>\n",
       "      <td>-0.021651</td>\n",
       "      <td>-0.307902</td>\n",
       "      <td>-0.443819</td>\n",
       "      <td>-0.391257</td>\n",
       "      <td>0.400275</td>\n",
       "      <td>0.207513</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>-0.793533</td>\n",
       "      <td>-0.903334</td>\n",
       "      <td>-0.252147</td>\n",
       "      <td>-0.451421</td>\n",
       "      <td>0.494141</td>\n",
       "      <td>-0.543820</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.288275</td>\n",
       "      <td>-0.916234</td>\n",
       "      <td>0.872236</td>\n",
       "      <td>-0.405840</td>\n",
       "      <td>0.887067</td>\n",
       "      <td>-0.890248</td>\n",
       "      <td>-0.080239</td>\n",
       "      <td>-0.253946</td>\n",
       "      <td>0.607473</td>\n",
       "      <td>0.890332</td>\n",
       "      <td>-0.331160</td>\n",
       "      <td>-0.502541</td>\n",
       "      <td>0.636328</td>\n",
       "      <td>-0.899832</td>\n",
       "      <td>0.891958</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>-0.235090</td>\n",
       "      <td>-0.606190</td>\n",
       "      <td>0.716390</td>\n",
       "      <td>-0.910605</td>\n",
       "      <td>-0.424803</td>\n",
       "      <td>-0.059388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.098515</td>\n",
       "      <td>-0.385608</td>\n",
       "      <td>-0.987904</td>\n",
       "      <td>0.494115</td>\n",
       "      <td>0.869421</td>\n",
       "      <td>-0.238608</td>\n",
       "      <td>-0.245373</td>\n",
       "      <td>0.274326</td>\n",
       "      <td>-0.947566</td>\n",
       "      <td>-0.950335</td>\n",
       "      <td>-0.018988</td>\n",
       "      <td>0.949433</td>\n",
       "      <td>-0.014700</td>\n",
       "      <td>0.939705</td>\n",
       "      <td>-0.249542</td>\n",
       "      <td>-0.130132</td>\n",
       "      <td>0.267692</td>\n",
       "      <td>-0.008533</td>\n",
       "      <td>0.047951</td>\n",
       "      <td>0.734438</td>\n",
       "      <td>0.511568</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-0.642022</td>\n",
       "      <td>0.352938</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.963973</td>\n",
       "      <td>-0.198282</td>\n",
       "      <td>-0.205098</td>\n",
       "      <td>0.149731</td>\n",
       "      <td>0.367538</td>\n",
       "      <td>0.343082</td>\n",
       "      <td>0.175255</td>\n",
       "      <td>-0.408339</td>\n",
       "      <td>-0.325079</td>\n",
       "      <td>-0.989443</td>\n",
       "      <td>-0.010332</td>\n",
       "      <td>0.279057</td>\n",
       "      <td>0.168347</td>\n",
       "      <td>-0.195600</td>\n",
       "      <td>-0.278317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652145</td>\n",
       "      <td>-0.299753</td>\n",
       "      <td>-0.139638</td>\n",
       "      <td>-0.173683</td>\n",
       "      <td>-0.443671</td>\n",
       "      <td>-0.027985</td>\n",
       "      <td>-0.349703</td>\n",
       "      <td>-0.388918</td>\n",
       "      <td>0.165237</td>\n",
       "      <td>0.035473</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>-0.882225</td>\n",
       "      <td>-0.953739</td>\n",
       "      <td>-0.245630</td>\n",
       "      <td>-0.413192</td>\n",
       "      <td>0.379641</td>\n",
       "      <td>-0.529157</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.287768</td>\n",
       "      <td>-0.922833</td>\n",
       "      <td>0.899929</td>\n",
       "      <td>-0.592388</td>\n",
       "      <td>0.953749</td>\n",
       "      <td>-0.889039</td>\n",
       "      <td>0.282065</td>\n",
       "      <td>-0.129607</td>\n",
       "      <td>0.664973</td>\n",
       "      <td>0.927450</td>\n",
       "      <td>-0.143531</td>\n",
       "      <td>-0.533200</td>\n",
       "      <td>0.266515</td>\n",
       "      <td>-0.961368</td>\n",
       "      <td>0.948298</td>\n",
       "      <td>-0.195683</td>\n",
       "      <td>-0.299023</td>\n",
       "      <td>-0.849391</td>\n",
       "      <td>0.298872</td>\n",
       "      <td>-0.932983</td>\n",
       "      <td>-0.178504</td>\n",
       "      <td>-0.309659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.180124</td>\n",
       "      <td>-0.413501</td>\n",
       "      <td>-0.984492</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>0.865759</td>\n",
       "      <td>-0.281325</td>\n",
       "      <td>-0.225921</td>\n",
       "      <td>0.305099</td>\n",
       "      <td>-0.935964</td>\n",
       "      <td>-0.974516</td>\n",
       "      <td>0.008402</td>\n",
       "      <td>0.932913</td>\n",
       "      <td>0.182340</td>\n",
       "      <td>0.928759</td>\n",
       "      <td>-0.195707</td>\n",
       "      <td>-0.154070</td>\n",
       "      <td>0.334482</td>\n",
       "      <td>-0.087378</td>\n",
       "      <td>0.104970</td>\n",
       "      <td>0.755661</td>\n",
       "      <td>0.508255</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-0.580859</td>\n",
       "      <td>0.430080</td>\n",
       "      <td>0.409025</td>\n",
       "      <td>0.948664</td>\n",
       "      <td>-0.238847</td>\n",
       "      <td>-0.117083</td>\n",
       "      <td>0.270822</td>\n",
       "      <td>0.436732</td>\n",
       "      <td>0.326114</td>\n",
       "      <td>0.217338</td>\n",
       "      <td>-0.518259</td>\n",
       "      <td>-0.354096</td>\n",
       "      <td>-0.987442</td>\n",
       "      <td>-0.171870</td>\n",
       "      <td>0.272495</td>\n",
       "      <td>0.096164</td>\n",
       "      <td>-0.216041</td>\n",
       "      <td>-0.242049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606793</td>\n",
       "      <td>-0.327609</td>\n",
       "      <td>-0.150611</td>\n",
       "      <td>-0.184259</td>\n",
       "      <td>-0.386002</td>\n",
       "      <td>-0.137363</td>\n",
       "      <td>-0.393250</td>\n",
       "      <td>-0.383060</td>\n",
       "      <td>0.206357</td>\n",
       "      <td>0.095422</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>-0.869633</td>\n",
       "      <td>-0.944107</td>\n",
       "      <td>-0.264682</td>\n",
       "      <td>-0.423192</td>\n",
       "      <td>0.436482</td>\n",
       "      <td>-0.529781</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.280890</td>\n",
       "      <td>-0.925910</td>\n",
       "      <td>0.897921</td>\n",
       "      <td>-0.559192</td>\n",
       "      <td>0.944515</td>\n",
       "      <td>-0.896119</td>\n",
       "      <td>0.235804</td>\n",
       "      <td>-0.174416</td>\n",
       "      <td>0.641793</td>\n",
       "      <td>0.917698</td>\n",
       "      <td>-0.214031</td>\n",
       "      <td>-0.523474</td>\n",
       "      <td>0.428578</td>\n",
       "      <td>-0.948966</td>\n",
       "      <td>0.931038</td>\n",
       "      <td>-0.180842</td>\n",
       "      <td>-0.408895</td>\n",
       "      <td>-0.821256</td>\n",
       "      <td>0.473733</td>\n",
       "      <td>-0.940421</td>\n",
       "      <td>-0.238981</td>\n",
       "      <td>-0.261727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2    ...       765       766       767\n",
       "0 -0.120891 -0.280994 -0.963245  ... -0.855487 -0.178540 -0.240451\n",
       "1 -0.183047 -0.320446 -0.967441  ... -0.887561 -0.124455 -0.239807\n",
       "2 -0.447093 -0.470325 -0.965326  ... -0.910605 -0.424803 -0.059388\n",
       "3 -0.098515 -0.385608 -0.987904  ... -0.932983 -0.178504 -0.309659\n",
       "4 -0.180124 -0.413501 -0.984492  ... -0.940421 -0.238981 -0.261727\n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final corpus\n",
    "corpus_bert_df=pd.DataFrame(final)\n",
    "corpus_bert_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc9EZDjlAMoP"
   },
   "outputs": [],
   "source": [
    "pickle.dump(corpus_bert_df, open(\"/content/drive/MyDrive/TWSM_Data/BertModel.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8C8XaaSrrb6T",
    "m1FM2CMTQf8w"
   ],
   "machine_shape": "hm",
   "name": "4_Text_Representation_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
